{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b35139b-433b-41e9-bf18-5e58178e08b9",
   "metadata": {},
   "source": [
    "# Kubeflow - VertexAI pipelines tutorial\n",
    "## Installing required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3885625-8520-4cc4-b72c-cc00a32ce585",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! pip3 install --no-cache-dir --upgrade \"kfp>2\" \\\n",
    "                                        google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b889e1f-ef59-47e5-8d15-7df416cd1938",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "! python3 -c \"import kfp; print('KFP SDK version: {}'.format(kfp.__version__))\"\n",
    "! pip3 freeze | grep aiplatform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64b2b717-e262-4fd1-a85a-48ada3c04a2d",
   "metadata": {},
   "source": [
    "## Define your values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5897595f-a01b-4e80-8e2b-79f39a361615",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import string\n",
    "PROJECT_ID = \"your-project-id\"\n",
    "LOCATION = \"us-central1\"\n",
    "random_suffix = ''.join(random.choices(string.ascii_lowercase + string.digits, k=8)) # Comenta esto y reemplaza con el valor que se imprime al ejecutar la celda para evitar multiples buckets\n",
    "print(\"Este es el valor a reemplazar en random_suffix: \"+str(random_suffix))\n",
    "\n",
    "BUCKET_NAME = f\"{PROJECT_ID}-bucket-{random_suffix}\"\n",
    "PIPELINE_ROOT = f\"gs://{BUCKET_NAME}/pipeline_root/\"\n",
    "\n",
    "BQ_LOCATION = LOCATION.split(\"-\")[0].upper()\n",
    "BUCKET_URI = \"gs://\"+BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c90267-3290-4e35-b512-cc0785c937d5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Service account\n",
    "shell_output = !gcloud auth list 2>/dev/null\n",
    "SERVICE_ACCOUNT = shell_output[2].replace(\"*\", \"\").strip()\n",
    "print(SERVICE_ACCOUNT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8feeb1d9-8211-4feb-a48c-5aa3ba8a877e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectCreator $BUCKET_URI\n",
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.objectViewer $BUCKET_URI\n",
    "!gsutil iam ch serviceAccount:{SERVICE_ACCOUNT}:roles/storage.admin $BUCKET_URI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95835bc8-1dc8-4acd-a4c5-8db3d2375b28",
   "metadata": {},
   "source": [
    "## Initialize Vertex AI pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bd3dfc-dfdd-4e34-8bd7-ecba59e69222",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import google.cloud.aiplatform as aiplatform\n",
    "import kfp\n",
    "from kfp import compiler, dsl\n",
    "from kfp.dsl import Artifact, Dataset, Input, Metrics, Model, Output, component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3591981-855b-475c-9da2-79578c393e75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, staging_bucket=BUCKET_URI)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "385a4e7f-60ef-478b-a762-9e565522dd35",
   "metadata": {},
   "source": [
    "# Exercise: build a training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8bbdee-419f-415a-8e30-fe7e6956513c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kfp import dsl\n",
    "from kfp.dsl import component, Input, Output, Dataset, Model\n",
    "from kfp.compiler import Compiler\n",
    "\n",
    "@component(packages_to_install=['scikit-learn', 'numpy'])\n",
    "def ingest_data(X_out: Output[Dataset], y_out: Output[Dataset]):\n",
    "    # Ejemplo del primer componente utilizando tipos Output y guardando outputs en disco en lugar de enviarlos directamente como una variable u objeto\n",
    "    from sklearn.datasets import load_iris\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    iris = load_iris()\n",
    "\n",
    "    os.makedirs(X_out.path, exist_ok=True)\n",
    "    os.makedirs(y_out.path, exist_ok=True)\n",
    "\n",
    "    np.save(f\"{X_out.path}/X.npy\", iris.data)\n",
    "    np.save(f\"{y_out.path}/y.npy\", iris.target)\n",
    "\n",
    "@component(packages_to_install=['scikit-learn', 'numpy'])\n",
    "def split_data(\n",
    "    X_in: # TODO,\n",
    "    y_in: # TODO,\n",
    "    X_train_out: # TODO,\n",
    "    X_test_out: # TODO,\n",
    "    y_train_out: # TODO,\n",
    "    y_test_out: # TODO,\n",
    "    test_size: float = 0.2\n",
    "):\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    import numpy as np\n",
    "    import os\n",
    "\n",
    "    # TODO: Carga los ficheros guardados en el componente anterior, recuerda utilizar el nombre del objeto y la variable correspondiente\n",
    "\n",
    "    # TODO: Train - test split\n",
    "\n",
    "    # TODO: Crea las carpetas correspondientes para evitar errores de escritura (fíjate en el componente anterior)\n",
    "\n",
    "    # TODO: Guarda los splits creados (fíjate en el componente anterior)\n",
    "    \n",
    "\n",
    "@component(packages_to_install=['scikit-learn', 'numpy'])\n",
    "def train(\n",
    "    X_train: # TODO,\n",
    "    y_train: # TODO,\n",
    "    model_out: # TODO,\n",
    "    max_depth: # TODO,\n",
    "    n_estimators: # TODO,\n",
    "    random_state: # TODO\n",
    "):\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    import pickle, numpy as np, os\n",
    "\n",
    "    # TODO: Carga X e Y de entrenamiento\n",
    "\n",
    "    # TODO: Entrena el modelo\n",
    "    \n",
    "    # TODO: Crea las carpeta correspondiente para evitar errores de escritura (fíjate en el componente anterior)\n",
    "\n",
    "    # TODO: Guarda el modelo entrenado\n",
    "\n",
    "@component(packages_to_install=['scikit-learn', 'numpy'])\n",
    "def show_metrics(\n",
    "    model: # TODO, \n",
    "    X_test: # TODO, \n",
    "    y_test: # TODO\n",
    "):\n",
    "    from sklearn.metrics import classification_report, confusion_matrix\n",
    "    import pickle, numpy as np\n",
    "\n",
    "    # TODO: Carga X e Y de test\n",
    "    \n",
    "    # TODO: Carga el modelo\n",
    "\n",
    "    # TODO: Predice y haz un print de las métricas correspondientes\n",
    "\n",
    "\n",
    "@dsl.pipeline\n",
    "def training_pipeline(max_depth: int = 2, n_estimators: int = 100, random_state: int = 0):\n",
    "    # Dos primeros componentes como ejemplo para los siguientes:\n",
    "    ingest = ingest_data()\n",
    "    \n",
    "    split = split_data(\n",
    "        X_in=ingest.outputs['X_out'],\n",
    "        y_in=ingest.outputs['y_out'],\n",
    "    )\n",
    "\n",
    "    model = train(\n",
    "        # TODO\n",
    "    )\n",
    "\n",
    "    show_metrics(\n",
    "        # TODO\n",
    "    )\n",
    "\n",
    "compiler.Compiler().compile(pipeline_func=training_pipeline, package_path='training_pipeline.yaml')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647f310b-edd5-4c15-8824-a339255fae0e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "job = aiplatform.PipelineJob(\n",
    "    display_name=\"training_pipeline\",\n",
    "    template_path=\"training_pipeline.yaml\",\n",
    "    pipeline_root=PIPELINE_ROOT,\n",
    ")\n",
    "\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de60a71-e813-42bb-90ec-ea8e02840d3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m127",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m127"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
