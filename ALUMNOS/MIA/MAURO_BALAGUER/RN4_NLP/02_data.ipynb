{"cells":[{"cell_type":"markdown","id":"d95f841a-63c9-41d4-aea1-496b3d2024dd","metadata":{"id":"d95f841a-63c9-41d4-aea1-496b3d2024dd"},"source":["**LLM Workshop 2024 by Sebastian Raschka**\n","\n","This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"]},{"cell_type":"markdown","id":"25aa40e3-5109-433f-9153-f5770531fe94","metadata":{"id":"25aa40e3-5109-433f-9153-f5770531fe94"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2) Understanding LLM Input Data"]},{"cell_type":"markdown","id":"76d5d2c0-cba8-404e-9bf3-71a218cae3cf","metadata":{"id":"76d5d2c0-cba8-404e-9bf3-71a218cae3cf"},"source":["Packages that are being used in this notebook:"]},{"cell_type":"code","execution_count":null,"id":"4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3","metadata":{"id":"4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3","outputId":"e4a75ba8-b2ad-40a2-f8a6-02eaf8fd7231"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch version: 2.2.1+cu121\n","tiktoken version: 0.7.0\n"]}],"source":["from importlib.metadata import version\n","\n","\n","print(\"torch version:\", version(\"torch\"))\n","print(\"tiktoken version:\", version(\"tiktoken\"))"]},{"cell_type":"markdown","id":"5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a","metadata":{"id":"5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a"},"source":["- This notebook provides a brief overview of the data preparation and sampling procedures to get input data \"ready\" for an LLM\n","- Understanding what the input data looks like is a great first step towards understanding how LLMs work"]},{"cell_type":"markdown","id":"628b2922-594d-4ff9-bd82-04f1ebdf41f5","metadata":{"id":"628b2922-594d-4ff9-bd82-04f1ebdf41f5"},"source":["<img src=\"https://camo.githubusercontent.com/590a463dcb825375473c9fd366013e86204589d68be0bd0207d43b158ba10558/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30312e776562703f74696d657374616d703d31\" width=\"700px\">"]},{"cell_type":"markdown","id":"eddbb984-8d23-40c5-bbfa-c3c379e7eec3","metadata":{"id":"eddbb984-8d23-40c5-bbfa-c3c379e7eec3"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.1 Tokenizing text"]},{"cell_type":"markdown","id":"f9c90731-7dc9-4cd3-8c4a-488e33b48e80","metadata":{"id":"f9c90731-7dc9-4cd3-8c4a-488e33b48e80"},"source":["- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"]},{"cell_type":"markdown","id":"09872fdb-9d4e-40c4-949d-52a01a43ec4b","metadata":{"id":"09872fdb-9d4e-40c4-949d-52a01a43ec4b"},"source":["<img src=\"https://camo.githubusercontent.com/b92bf8c18c5d51258b4a8a55d9612fd1a2eb5f3b6c6a79fc0a1d7b1ba59a2e99/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30342e77656270\" width=\"600px\">"]},{"cell_type":"markdown","source":["Enlace a texto: https://drive.google.com/file/d/1H_ZU_35t3sqg9LklLau-twiWKNQraonx/view?usp=sharing"],"metadata":{"id":"XalQquvs1uBy"},"id":"XalQquvs1uBy"},{"cell_type":"markdown","id":"8cceaa18-833d-46b6-b211-b20c53902805","metadata":{"id":"8cceaa18-833d-46b6-b211-b20c53902805"},"source":["- Load raw text we want to work with\n","- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"]},{"cell_type":"code","execution_count":null,"id":"8a769e87-470a-48b9-8bdb-12841b416198","metadata":{"id":"8a769e87-470a-48b9-8bdb-12841b416198","outputId":"1cbd4963-8fcd-44d0-fca1-96bb96b375d4"},"outputs":[{"name":"stdout","output_type":"stream","text":["Total number of character: 20479\n","I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"]}],"source":["with open(\"LLM-workshop-2024/02_data/the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n","    raw_text = f.read()\n","\n","print(\"Total number of character:\", len(raw_text))\n","print(raw_text[:99])"]},{"cell_type":"markdown","id":"9b971a46-ac03-4368-88ae-3f20279e8f4e","metadata":{"id":"9b971a46-ac03-4368-88ae-3f20279e8f4e"},"source":["- The goal is to tokenize and embed this text for an LLM\n","- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above"]},{"cell_type":"markdown","id":"6cbe9330-b587-4262-be9f-497a84ec0e8a","metadata":{"id":"6cbe9330-b587-4262-be9f-497a84ec0e8a"},"source":["<img src=\"https://camo.githubusercontent.com/241f7a302c33bc1e8156e7d0b153caae8728f2c9cd03884487c05d931fd88be2/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30352e77656270\" width=\"600px\">"]},{"cell_type":"markdown","id":"3daa1687-2c08-485a-87cc-a93c2f9586d7","metadata":{"id":"3daa1687-2c08-485a-87cc-a93c2f9586d7"},"source":["- The following regular expression will split on whitespaces and punctuation"]},{"cell_type":"markdown","id":"3588a341","metadata":{"id":"3588a341"},"source":["#### Ejercicio\n","\n","Crea el código que permita dividir el texto siempre que haya uno de los siguientes caracteres empleando expresiones regulares:\n","\n","- ,\n","- .\n","- :\n","- ;\n","- ?\n","- _\n","- !\n","- \"\n","- (\n","- )\n","- '\n","\n","o bien:\n","\n","- \"--\"\n","\n","o bien:\n","\n","- \" \"\n"]},{"cell_type":"code","execution_count":null,"id":"737dd5b0-9dbb-4a97-9ae4-3482c8c04be7","metadata":{"id":"737dd5b0-9dbb-4a97-9ae4-3482c8c04be7"},"outputs":[],"source":["import re\n","\n","preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n","preprocessed = [item for item in preprocessed if item]\n","print(preprocessed[:38])"]},{"cell_type":"code","execution_count":null,"id":"35db7b5e-510b-4c45-995f-f5ad64a8e19c","metadata":{"id":"35db7b5e-510b-4c45-995f-f5ad64a8e19c"},"outputs":[],"source":["print(\"Number of tokens:\", len(preprocessed))"]},{"cell_type":"markdown","id":"0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231","metadata":{"id":"0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.2 Converting tokens into token IDs"]},{"cell_type":"markdown","id":"a5204973-f414-4c0d-87b0-cfec1f06e6ff","metadata":{"id":"a5204973-f414-4c0d-87b0-cfec1f06e6ff"},"source":["- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n","- For this we first need to build a vocabulary"]},{"cell_type":"markdown","id":"177b041d-f739-43b8-bd81-0443ae3a7f8d","metadata":{"id":"177b041d-f739-43b8-bd81-0443ae3a7f8d"},"source":["<img src=\"https://camo.githubusercontent.com/bf01ba4b1b924633325cda845feac84ae1a3f154db5098f5d70e90470ff4484e/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30362e77656270\" width=\"900px\">"]},{"cell_type":"markdown","id":"8eeade64-037b-4b59-9039-d3b000ef8886","metadata":{"id":"8eeade64-037b-4b59-9039-d3b000ef8886"},"source":["- The vocabulary contains the unique words in the input text"]},{"cell_type":"code","execution_count":null,"id":"7fdf0533-5ab6-42a5-83fa-a3b045de6396","metadata":{"id":"7fdf0533-5ab6-42a5-83fa-a3b045de6396","outputId":"10c4521a-60f1-419a-9f77-efacfff5e06e"},"outputs":[{"name":"stdout","output_type":"stream","text":["1132\n"]}],"source":["all_words = sorted(set(preprocessed))\n","vocab_size = len(all_words)\n","\n","print(vocab_size)"]},{"cell_type":"markdown","id":"b636d953","metadata":{"id":"b636d953"},"source":["#### Ejercicio\n","\n","A continuación deberéis crear el vocabulario. La forma más sencilla es emplear `all_words` para crear un diccinario en la forma: `{palabra1: 0, palabra2: 1, ...}`."]},{"cell_type":"code","execution_count":null,"id":"77d00d96-881f-4691-bb03-84fec2a75a26","metadata":{"id":"77d00d96-881f-4691-bb03-84fec2a75a26"},"outputs":[],"source":["vocab = {token:integer for integer,token in enumerate(all_words)}"]},{"cell_type":"markdown","id":"75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3","metadata":{"id":"75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3"},"source":["- Below are the first 50 entries in this vocabulary:"]},{"cell_type":"code","execution_count":null,"id":"e1c5de4a-aa4e-4aec-b532-10bb364039d6","metadata":{"id":"e1c5de4a-aa4e-4aec-b532-10bb364039d6"},"outputs":[],"source":["for i, item in enumerate(vocab.items()):\n","    print(item)\n","    if i >= 50:\n","        break"]},{"cell_type":"markdown","id":"3b1dc314-351b-476a-9459-0ec9ddc29b19","metadata":{"id":"3b1dc314-351b-476a-9459-0ec9ddc29b19"},"source":["- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"]},{"cell_type":"markdown","id":"67407a9f-0202-4e7c-9ed7-1b3154191ebc","metadata":{"id":"67407a9f-0202-4e7c-9ed7-1b3154191ebc"},"source":["<img src=\"https://camo.githubusercontent.com/8955d3aea45dc06f156d0579f7f3302c27b6635e649c301dbab33427b2d8d2a8/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30372e776562703f313233\" width=\"600px\">"]},{"cell_type":"markdown","id":"4e569647-2589-4c9d-9a5c-aef1c88a0a9a","metadata":{"id":"4e569647-2589-4c9d-9a5c-aef1c88a0a9a"},"source":["- Let's now put it all together into a tokenizer class"]},{"cell_type":"markdown","id":"1ff243e4","metadata":{"id":"1ff243e4"},"source":["Cómo podréis imaginaros, en la vida real esto no se hace con un jupyter notebook y con funciones, sino que se programan clases que puedan organizar y modularizar el código para poder reaprovecharlo.\n","\n","A continuación crearéis una clase llamada `SimpleTokenizerV1` que tendrá un método `__init__(vocab)` que permitirá iniciar el vocabulario para poder convertir de caracter a ID (int) y de ID (int) a caracter.\n","\n","Además, tendrá también un método `encode(self, text)` que se encargará de devolver las IDs del texto que tiene como entrada. La implementación de este método es muy sencilla si os basáis en lo hecho previamente.\n","\n","Por último, habrá un método `decode(self, ids)` que permitirá convertir de IDs a texto. En este método tendréis que emplear la variable `self.int_to_str` para pasar de IDs a caracteres, y luego concatenar todos los caracteres para obtener una cadena de texto.\n","\n","La última línea, `text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)`, se encarga de eliminar los espacios **previos** a los símbolos indicados en la primera cadena del `re.sub(...)`."]},{"cell_type":"code","execution_count":null,"id":"f531bf46-7c25-4ef8-bff8-0d27518676d5","metadata":{"id":"f531bf46-7c25-4ef8-bff8-0d27518676d5"},"outputs":[],"source":["class SimpleTokenizerV1:\n","    def __init__(self, vocab):\n","        self.str_to_int = vocab\n","        self.int_to_str = {i:s for s,i in vocab.items()}\n","\n","    def encode(self, text):\n","        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n","        preprocessed = [\n","            item.strip() for item in preprocessed if item.strip()\n","        ]\n","        ids = [self.str_to_int[s] for s in preprocessed]\n","        return ids\n","\n","    def decode(self, ids):\n","        text = \" \".join([self.int_to_str[i] for i in ids])\n","        # Replace spaces before the specified punctuations\n","        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n","        return text"]},{"cell_type":"markdown","id":"dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7","metadata":{"id":"dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7"},"source":["- The `encode` function turns text into token IDs\n","- The `decode` function turns token IDs back into text"]},{"cell_type":"markdown","id":"cc21d347-ec03-4823-b3d4-9d686e495617","metadata":{"id":"cc21d347-ec03-4823-b3d4-9d686e495617"},"source":["<img src=\"https://camo.githubusercontent.com/b324e29fe9d3d4191a9200d6a08983eef4d3f835cff85ce1ee4aceb47117891a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30382e776562703f313233\" width=\"600px\">"]},{"cell_type":"markdown","id":"c2950a94-6b0d-474e-8ed0-66d0c3c1a95c","metadata":{"id":"c2950a94-6b0d-474e-8ed0-66d0c3c1a95c"},"source":["- We can use the tokenizer to encode (that is, tokenize) texts into integers\n","- These integers can then be embedded (later) as input of/for the LLM"]},{"cell_type":"code","execution_count":null,"id":"647364ec-7995-4654-9b4a-7607ccf5f1e4","metadata":{"id":"647364ec-7995-4654-9b4a-7607ccf5f1e4"},"outputs":[],"source":["tokenizer = SimpleTokenizerV1(vocab)\n","\n","text = \"\"\"\"It's the last he painted, you know,\"\n","           Mrs. Gisburn said with pardonable pride.\"\"\"\n","ids = tokenizer.encode(text)\n","print(ids)"]},{"cell_type":"markdown","id":"3201706e-a487-4b60-b99d-5765865f29a0","metadata":{"id":"3201706e-a487-4b60-b99d-5765865f29a0"},"source":["- We can decode the integers back into text"]},{"cell_type":"code","execution_count":null,"id":"01d8c8fb-432d-4a49-b332-99f23b233746","metadata":{"id":"01d8c8fb-432d-4a49-b332-99f23b233746"},"outputs":[],"source":["tokenizer.decode(ids)"]},{"cell_type":"code","execution_count":null,"id":"54f6aa8b-9827-412e-9035-e827296ab0fe","metadata":{"id":"54f6aa8b-9827-412e-9035-e827296ab0fe"},"outputs":[],"source":["tokenizer.decode(tokenizer.encode(text))"]},{"cell_type":"markdown","id":"002745fd","metadata":{"id":"002745fd"},"source":["Si todo ha ido bien, estas dos últimas celdas deberían devolver la misma cadena que hay disponible en la variable `text`."]},{"cell_type":"markdown","id":"5c4ba34b-170f-4e71-939b-77aabb776f14","metadata":{"id":"5c4ba34b-170f-4e71-939b-77aabb776f14"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.3 BytePair encoding"]},{"cell_type":"markdown","id":"ee3926e2","metadata":{"id":"ee3926e2"},"source":["Byte-Pair Encoding (BPE) es una técnica que permite codificar el contenido a nivel de sub-palabra. Su método básico de funcionamiento es:\n","\n","1. Se parte de un vocabulario con todos los caracteres individuales\n","2. Se van incorporando nuevos pares de tokens basándose en su mayor frecuencia de aparición en el texto a tokenizar\n","3. Así hasta llegar a un tamaño de vocabulario definido (hiperparámetro)\n","\n","Por ejemplo:\n","\n","1. Partimos del vocabulario: `u, g, n`\n","2. Se crean los siguientes pares:\n","\n","```\n","(\"u\", \"g\") -> \"ug\"\n","(\"u\", \"n\") -> \"un\"\n","(\"h\", \"ug\") -> \"hug\"\n","```\n","\n","La palabra \"bug\" será tokenizada como [\"b\", \"ug\"]. En cambio, \"mug\", será tokenizado como [\"[UNK]\", \"ug\"] dado que la letra \"m\" no fue parte del vocabulario base. De la misma manera, la palabra \"thug\" será tokenizada como [\"[UNK]\", \"hug\"]: la letra \"t\" no está en el vocabulario base, y aplicando las reglas de fusión resulta primero la fusión de \"u\" y \"g\" y luego de \"hu\" and \"g\".\n","\n","Se trata de un método de tokenización mucho más sofisticado que proporciona mayor velocidad. De hecho, es el tokenizador empleado para entrenar GPT-2, GPT-3, y ChatGPT, entre otros.\n","\n","Aquí podéis ver un tutorial en el que explican detalladamente su funcionamiento: https://huggingface.co/learn/nlp-course/es/chapter6/5.\n","\n","Y aquí otro de Sebastian Raschka en el que explica su implementación paso a paso: https://sebastianraschka.com/blog/2025/bpe-from-scratch.html.\n","\n","Como su implementación puede resultar complicada, vamos a emplear una librería open-source disponible en Python: `tiktoken` (https://github.com/openai/tiktoken), que implementa el algoritmo BPE de forma muy eficiente en Rust.\n"]},{"cell_type":"markdown","id":"2309494c-79cf-4a2d-bc28-a94d602f050e","metadata":{"id":"2309494c-79cf-4a2d-bc28-a94d602f050e"},"source":["More info:\n","- GPT-2 used BytePair encoding (BPE) as its tokenizer\n","- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n","- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n","- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n","- In this lecture, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n","- (Based on an analysis [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb), I found that `tiktoken` is approx. 3x faster than the original tokenizer and 6x faster than an equivalent tokenizer in Hugging Face)"]},{"cell_type":"code","execution_count":null,"id":"ede1d41f-934b-4bf4-8184-54394a257a94","metadata":{"id":"ede1d41f-934b-4bf4-8184-54394a257a94"},"outputs":[],"source":["# pip install tiktoken"]},{"cell_type":"code","execution_count":null,"id":"48967a77-7d17-42bf-9e92-fc619d63a59e","metadata":{"id":"48967a77-7d17-42bf-9e92-fc619d63a59e"},"outputs":[],"source":["import importlib\n","import tiktoken\n","\n","print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"]},{"cell_type":"code","execution_count":null,"id":"6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128","metadata":{"id":"6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128"},"outputs":[],"source":["tokenizer = tiktoken.get_encoding(\"gpt2\")"]},{"cell_type":"code","execution_count":null,"id":"5ff2cd85-7cfb-4325-b390-219938589428","metadata":{"id":"5ff2cd85-7cfb-4325-b390-219938589428"},"outputs":[],"source":["text = (\n","    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n","     \"of someunknownPlace.\"\n",")\n","\n","integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n","\n","print(integers)"]},{"cell_type":"code","execution_count":null,"id":"d26a48bb-f82e-41a8-a955-a1c9cf9d50ab","metadata":{"id":"d26a48bb-f82e-41a8-a955-a1c9cf9d50ab"},"outputs":[],"source":["strings = tokenizer.decode(integers)\n","\n","print(strings)"]},{"cell_type":"markdown","id":"e8c2e7b4-6a22-42aa-8e4d-901f06378d4a","metadata":{"id":"e8c2e7b4-6a22-42aa-8e4d-901f06378d4a"},"source":["- BPE tokenizers break down unknown words into subwords and individual characters:"]},{"cell_type":"markdown","id":"c082d41f-33d7-4827-97d8-993d5a84bb3c","metadata":{"id":"c082d41f-33d7-4827-97d8-993d5a84bb3c"},"source":["<img src=\"https://camo.githubusercontent.com/5938dff392e5cb7404d2636e4d7157fceb4c36ecf57a2173001bd3edf22234da/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31312e77656270\" width=\"600px\">"]},{"cell_type":"code","execution_count":null,"id":"0beb27ee-1156-457c-839e-eebb48d94d0e","metadata":{"id":"0beb27ee-1156-457c-839e-eebb48d94d0e"},"outputs":[],"source":["tokenizer.encode(\"Akwirw ier\", allowed_special={\"<|endoftext|>\"})"]},{"cell_type":"markdown","id":"634a8208","metadata":{"id":"634a8208"},"source":["This code allows the special token \"*<|endoftext|>*\" to be encoded as a special token if it appears in the input text. However, since \"Akwirw ier\" doesn't contain \"<|endoftext|>\", the allowed_special parameter won't have any effect on the encoding of this specific input.\n","\n","The `allowed_special` parameter is particularly useful when you want to include certain special tokens in your input without raising errors or having them split into regular tokens.\n","\n","It's a safety measure to prevent accidental encoding of special tokens that might have unintended effects on model behavior."]},{"cell_type":"markdown","id":"abbd7c0d-70f8-4386-a114-907e96c950b0","metadata":{"id":"abbd7c0d-70f8-4386-a114-907e96c950b0"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# 2.4 Data sampling with a sliding window"]},{"cell_type":"markdown","id":"ec9958f7","metadata":{"id":"ec9958f7"},"source":["Por último, vamos a abordar el formato de los datos de entrenamiento. En el caso de los modelos GPT (Generative Pre-Trained models), el modelo va a aprender a predecir la siguiente palabra dada una secuencia de entrada.\n","\n","Por tanto, los datos de entrenamiento serán como podéis ver en la siquiente imagen."]},{"cell_type":"markdown","id":"39fb44f4-0c43-4a6a-9c2f-9cf31452354c","metadata":{"id":"39fb44f4-0c43-4a6a-9c2f-9cf31452354c"},"source":["<img src=\"https://camo.githubusercontent.com/b6245f4e6c64740c06f71ddd30d6495342b37315f0fd3556a0dc511be009a61f/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31322e77656270\" width=\"600px\">"]},{"cell_type":"markdown","id":"0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c","metadata":{"id":"0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c"},"source":["- For this, we use a sliding window approach, changing the position by +1:\n","\n","Para ello usaremos una ventana deslizante (*sliding window*) que nos permita ir recorriendo la totalidad del texto.\n","\n","Al final, tendremos un lote (*batch*) de datos tal que así:\n","\n","```\n","input_batch = [\n","    palabra1 palabra2 palabra3\n","    palabra2 palabra3 palabra4\n","    ...\n","    palabraN-2 palabraN-1 palabraN\n","]\n","```\n","\n","<img src=\"https://camo.githubusercontent.com/9c738e75095f70d3dc4f6b3630008dd67607b5fa92e3bf776b0ed2cbb68db299/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31332e776562703f313233\" width=\"900px\">"]},{"cell_type":"markdown","id":"b006212f-de45-468d-bdee-5806216d1679","metadata":{"id":"b006212f-de45-468d-bdee-5806216d1679"},"source":["Sin embargo, si os fijáis, estamos repitiendo las mismas palabras muchas veces, por lo que en realidad lo que se suele hacer es tener un \"salto\" (*stride*) igual a la longitud del vector de contexto (*context vector*).\n","\n","Tened en cuenta que estamos hablando de los inputs, las etiquetas seguirán siendo siempre la siguiente palabra para cada elemento del batch:\n","\n","```\n","input_batch = [\n","    \"<start-of-sequence>\" palabra1 palabra2\n","    palabra1 palabra2 palabra3\n","    palabra2 palabra3 palabra4\n","    ...\n","    palabraN-2 palabraN-1 palabraN\n","]\n","```\n","\n","```\n","targets = [\n","    palabra3\n","    palabra4\n","    palabra5\n","    ...\n","    \"<enf-of-sequence>\"\n","]\n","```"]},{"cell_type":"markdown","id":"9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3","metadata":{"id":"9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3"},"source":["<img src=\"https://camo.githubusercontent.com/181fa38c6bcf2259633e9a15874d189bf7a9f5ec1ca7b161f521a03ed27ec086/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31342e77656270\" width=\"600px\">"]},{"cell_type":"markdown","id":"fb83ce30","metadata":{"id":"fb83ce30"},"source":["Let's first have a look at our text:"]},{"cell_type":"code","execution_count":null,"id":"68becc7c","metadata":{"id":"68becc7c"},"outputs":[],"source":["print(raw_text[:100])"]},{"cell_type":"markdown","id":"81001555","metadata":{"id":"81001555"},"source":["Now, we will use the function `create_dataloader_v1` in `supplementary.py` to create a `DataLoader`, a Python object that will allow us to load the data efficiently to train our model."]},{"cell_type":"markdown","id":"629aa061","metadata":{"id":"629aa061"},"source":["Se trata de código estándar de Python para crear un DataLoader, aquí lo podéis ver también:\n","\n","```\n","def create_dataloader_v1(txt, batch_size=4, max_length=256,\n","                         stride=128, shuffle=True, drop_last=True,\n","                         num_workers=0):\n","\n","    # Initialize the tokenizer\n","    tokenizer = tiktoken.get_encoding(\"gpt2\")\n","\n","    # Create dataset\n","    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n","\n","    # Create dataloader\n","    dataloader = DataLoader(\n","        dataset,\n","        batch_size=batch_size,\n","        shuffle=shuffle,\n","        drop_last=drop_last,\n","        num_workers=num_workers\n","    )\n","\n","    return dataloader\n","```"]},{"cell_type":"code","execution_count":null,"id":"fb55f51a","metadata":{"id":"fb55f51a"},"outputs":[],"source":["from supplementary import create_dataloader_v1\n","\n","\n","dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"\\nTargets:\\n\", targets)"]},{"cell_type":"markdown","id":"28c190ad","metadata":{"id":"28c190ad"},"source":["Veamos el contenido de las `inputs`:"]},{"cell_type":"code","execution_count":null,"id":"c37d55e3","metadata":{"id":"c37d55e3"},"outputs":[],"source":["for vector in inputs:\n","    strings = tokenizer.decode(vector.numpy())\n","    print(strings)"]},{"cell_type":"markdown","id":"a9b85859","metadata":{"id":"a9b85859"},"source":["Y ahora de los `targets`:"]},{"cell_type":"code","execution_count":null,"id":"4a102b63","metadata":{"id":"4a102b63"},"outputs":[],"source":["for vector in targets:\n","    strings = tokenizer.decode(vector.numpy())\n","    print(strings)"]},{"cell_type":"markdown","id":"a5e402d0","metadata":{"id":"a5e402d0"},"source":["**¿Notáis algo extraño?**\n","\n","Según os he dicho, en targets está la palabra a predecir, ¿no? ¿Por qué, entonces, tiene tamaño 4, y no 1?\n"]},{"cell_type":"markdown","id":"fb3fa1e7","metadata":{"id":"fb3fa1e7"},"source":["Esto es así porque los transformers predicen para cada token de entrada, el probable siguiente token, y lo hacen **a la vez para todos los tokens**. De ahí que digamos que tienen en cuenta el contexto de la *context window*. Es decir, al final esta es la realidad:\n","\n","```\n","input_batch = [\n","    \"<start-of-sequence>\" palabra1 palabra2\n","    palabra1 palabra2 palabra3\n","    palabra2 palabra3 palabra4\n","    ...\n","    palabraN-2 palabraN-1 palabraN\n","]\n","```\n","\n","```\n","targets = [\n","    palabra1 palabra2 palabra3\n","    palabra2 palabra3 palabra4\n","    palabra3 palabra4 palabra5\n","    ...\n","    palabraN-1 palabraN \"<enf-of-sequence>\"\n","]\n","```"]},{"cell_type":"markdown","id":"96a1c0c5","metadata":{"id":"96a1c0c5"},"source":["Ahora lo haremos con `stride=1` para ver la diferencia:"]},{"cell_type":"code","execution_count":null,"id":"d0ae9c2d","metadata":{"id":"d0ae9c2d"},"outputs":[],"source":["from supplementary import create_dataloader_v1\n","\n","\n","dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=1, shuffle=False)\n","\n","data_iter = iter(dataloader)\n","inputs, targets = next(data_iter)\n","print(\"Inputs:\\n\", inputs)\n","print(\"\\nTargets:\\n\", targets)"]},{"cell_type":"code","execution_count":null,"id":"f52533f1","metadata":{"id":"f52533f1"},"outputs":[],"source":["for vector in inputs:\n","    strings = tokenizer.decode(vector.numpy())\n","    print(strings)"]},{"cell_type":"code","execution_count":null,"id":"a374a418","metadata":{"id":"a374a418"},"outputs":[],"source":["for vector in targets:\n","    strings = tokenizer.decode(vector.numpy())\n","    print(strings)"]},{"cell_type":"markdown","id":"106f3ef6","metadata":{"id":"106f3ef6"},"source":["Fijaos como las palabras se dividen en sub-palabras:"]},{"cell_type":"code","execution_count":null,"id":"ec5b33a2","metadata":{"id":"ec5b33a2","outputId":"81e61941-7852-4a71-c094-e67512d78a57"},"outputs":[{"name":"stdout","output_type":"stream","text":["tensor([ 367, 2885, 1464, 1807])\n"]}],"source":["print(inputs[1])"]},{"cell_type":"code","execution_count":null,"id":"2f6feee5","metadata":{"id":"2f6feee5","outputId":"1a1522d0-3ff4-4637-d68d-152bf1223663"},"outputs":[{"name":"stdout","output_type":"stream","text":["token #0: ' H'\n","token #1: 'AD'\n","token #2: ' always'\n","token #3: ' thought'\n"]}],"source":["for i, token in enumerate(inputs[1]):\n","    print(f\"token #{i}: '{tokenizer.decode([token.numpy()])}'\")"]},{"cell_type":"markdown","id":"635e6125","metadata":{"id":"635e6125"},"source":["# **Notáis algo interesante?**"]},{"cell_type":"markdown","id":"8bb7493b","metadata":{"id":"8bb7493b"},"source":["¡Fijáos en que estamos codificando los espacios! Los modelos GPT codifican los espacios como un símbolo especial (^G). Esto es dependiente del modelo, por ejemplo, BERT no codifica los espacios. Sin embargo, ambos codifican los símbolos de puntuación. Pensad que tienen que ser capaces de reconstruir el texto original, incluyendo espacios y símbolos de puntuación.\n","\n","Ejemplo de tokenización de: \"Hello, how are you?\"\n","\n","BERT: `[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]`\n","\n","GPT: `[('Hello', (0, 5)), (',', (5, 6)), ('Ġhow', (6, 10)), ('Ġare', (10, 14)), ('Ġ', (14, 15)), ('Ġyou', (15, 19)),\n"," ('?', (19, 20))]`"]},{"cell_type":"code","execution_count":null,"id":"85b528b5","metadata":{"id":"85b528b5"},"outputs":[],"source":[]},{"cell_type":"markdown","id":"2dc671fb-6945-4594-b33f-8b462a69720d","metadata":{"id":"2dc671fb-6945-4594-b33f-8b462a69720d"},"source":["<br>\n","<br>\n","<br>\n","<br>\n","\n","# Ejercicio **evaluable**: Prepara tu dataset favorito"]},{"cell_type":"markdown","id":"e8257cc7","metadata":{"id":"e8257cc7"},"source":["Si quieres probar con algo similar a `the-verdict.txt` pero en español, puedes usar los disponibles en este dataset de HuggingFace: https://huggingface.co/datasets/Fernandoefg/cuentos_es (aquí más info: https://www.linkedin.com/pulse/dataset-de-cuentos-en-espa%C3%B1ol-fernando-fuentes-gallegos-ssuyc/)."]},{"cell_type":"markdown","id":"c272b2a9","metadata":{"id":"c272b2a9"},"source":[]},{"cell_type":"code","execution_count":null,"id":"12f65547","metadata":{"id":"12f65547"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}