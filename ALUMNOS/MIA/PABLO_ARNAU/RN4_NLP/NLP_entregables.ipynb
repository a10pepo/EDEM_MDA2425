{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **EJERCICIO 1**"
      ],
      "metadata": {
        "id": "r5MFPdttS6fg"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMPP3hVMqyyn"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"articuloIA.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HXM4i-SnOkw_",
        "outputId": "deb3a085-234f-4951-b405-62067ddfff54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 5423\n",
            "Revolución en la logística: La inteligencia artificial redefine la sostenibilidad en el transporte \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
        "preprocessed = [item for item in preprocessed if item]\n",
        "print(preprocessed[:38])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rlkMM30sOvq6",
        "outputId": "29bea9e6-cd8f-42fa-d79b-25b412d2a892"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Revolución', ' ', 'en', ' ', 'la', ' ', 'logística', ':', ' ', 'La', ' ', 'inteligencia', ' ', 'artificial', ' ', 'redefine', ' ', 'la', ' ', 'sostenibilidad', ' ', 'en', ' ', 'el', ' ', 'transporte', ' ', 'de', ' ', 'mercancías', '\\n', '\\n', 'Resulta', ' ', 'crucial', ' ', 'estar', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of tokens:\", len(preprocessed))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DIaWXIzcO0A_",
        "outputId": "6ce6a2ab-c157-45c0-8e25-d37d0f8b14c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of tokens: 1715\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "all_words = sorted(set(preprocessed))\n",
        "vocab_size = len(all_words)\n",
        "\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6iCO-VNO7x5",
        "outputId": "d7053745-b329-4716-8e2b-d18679101a76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "369\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {token:integer for integer,token in enumerate(all_words)}"
      ],
      "metadata": {
        "id": "VsqA5IJ3PFfo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, item in enumerate(vocab.items()):\n",
        "    print(item)\n",
        "    if i >= 50:\n",
        "        break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfIXbtYxO_zq",
        "outputId": "0d384064-b12e-4cf8-e40a-4e4c782f723f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('\\n', 0)\n",
            "(' ', 1)\n",
            "('(', 2)\n",
            "(')', 3)\n",
            "(',', 4)\n",
            "('.', 5)\n",
            "(':', 6)\n",
            "('A', 7)\n",
            "('Además', 8)\n",
            "('Al', 9)\n",
            "('Artificial', 10)\n",
            "('DB', 11)\n",
            "('El', 12)\n",
            "('En', 13)\n",
            "('Estas', 14)\n",
            "('Esto', 15)\n",
            "('Hemos', 16)\n",
            "('Hoy', 17)\n",
            "('IA', 18)\n",
            "('Inteligencia', 19)\n",
            "('La', 20)\n",
            "('Las', 21)\n",
            "('Los', 22)\n",
            "('Mediante', 23)\n",
            "('No', 24)\n",
            "('Otro', 25)\n",
            "('Resulta', 26)\n",
            "('Revolución', 27)\n",
            "('Schenker', 28)\n",
            "('Sin', 29)\n",
            "('Somos', 30)\n",
            "('Tradicionalmente', 31)\n",
            "('Una', 32)\n",
            "('Uno', 33)\n",
            "('a', 34)\n",
            "('accidentes', 35)\n",
            "('adopción', 36)\n",
            "('agilidad', 37)\n",
            "('ajustar', 38)\n",
            "('al', 39)\n",
            "('algoritmos', 40)\n",
            "('almacenamiento', 41)\n",
            "('altamente', 42)\n",
            "('alternativas', 43)\n",
            "('alto', 44)\n",
            "('analizar', 45)\n",
            "('ante', 46)\n",
            "('antes', 47)\n",
            "('análisis', 48)\n",
            "('aplicaciones', 49)\n",
            "('aprendizaje', 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleTokenizerV1():\n",
        "\n",
        "    def __init__(self, vocab):\n",
        "        self.str_to_int = vocab\n",
        "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
        "\n",
        "    def encode(self, text):\n",
        "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
        "        preprocessed = [\n",
        "            item.strip() for item in preprocessed if item.strip()\n",
        "        ]\n",
        "        ids = [self.str_to_int[s] for s in preprocessed]\n",
        "        return ids\n",
        "\n",
        "    def decode(self, ids):\n",
        "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
        "        # Replace spaces before the specified punctuations\n",
        "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
        "        return text"
      ],
      "metadata": {
        "id": "sjrwGTxJPWcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = '''Resulta crucial estar comprometidos con la adopción y el desarrollo de soluciones de IA que mejoren la calidad de los servicios\n",
        "Las empresas deben garantizar que el manejo de esta información se realice de manera ética y conforme a las normativas vigentes en materia de protección de datos\n",
        "Una de las aplicaciones más destacadas de la IA en el ámbito logístico es la optimización de rutas'''\n",
        "\n",
        "ids = tokenizer.encode(text)\n",
        "ids = torch.tensor(ids).unsqueeze(0)\n",
        "print(ids)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-JoLUk-RR_i0",
        "outputId": "d4becc51-0ae7-4184-a657-3dd3366d760d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 26, 103, 143,  78,  80, 196,  36, 361, 127, 120, 112, 332, 112,  18,\n",
            "         289, 217, 196,  64, 112, 204, 324,  21, 132, 113, 167, 289, 127, 206,\n",
            "         112, 141, 183, 317, 292, 112, 207, 367, 361,  84,  34, 198, 230, 358,\n",
            "         133, 210, 112, 286, 112, 111,  32, 112, 198,  49, 227, 122, 112, 196,\n",
            "          18, 133, 127, 364, 203, 137, 196, 251, 112, 314]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = SimpleTokenizerV1(vocab)\n",
        "\n",
        "text = '''IA en la logística optimiza'''\n",
        "ids2 = tokenizer.encode(text)\n",
        "print(ids2)\n",
        "ids2 = torch.tensor(ids2).unsqueeze(0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dc7a89e-be38-4222-bbc2-80965176e6d1",
        "id": "iCTu9rKyYFtq"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[18, 133, 196, 201, 250]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EJERCICIO 2**\n"
      ],
      "metadata": {
        "id": "s2xoARh5ThVP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class SimpleLSTMModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(SimpleLSTMModel, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)   # Convierte IDs en vectores\n",
        "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)            # Salida: logits para cada token\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Entrada:\n",
        "            x: Tensor de forma (batch_size, sequence_length)\n",
        "        Salida:\n",
        "            logits: Tensor de forma (batch_size, sequence_length, vocab_size)\n",
        "        \"\"\"\n",
        "        x = self.embedding(x)              # (batch_size, seq_len, embed_size)\n",
        "        out, _ = self.lstm(x)              # (batch_size, seq_len, hidden_size)\n",
        "        logits = self.fc(out)              # (batch_size, seq_len, vocab_size)\n",
        "        return logits\n"
      ],
      "metadata": {
        "id": "rHaV8CpPcBUT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)  # (batch, vocab_size)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ],
      "metadata": {
        "id": "A5NfQXTOUv-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USAMOS PARA GENERAR TEXTO UNA LSTM BÁSICA CON NUESTRO VOCABULARIO GENERADO EN EL EJERCICIO 1"
      ],
      "metadata": {
        "id": "L5-OTW5hWEcg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = SimpleLSTMModel(vocab_size, embed_size=64,hidden_size=128)"
      ],
      "metadata": {
        "id": "PZrCqUdGcQ1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output = generate_text_simple(model, idx = ids2 , max_new_tokens=20, context_size= 100)"
      ],
      "metadata": {
        "id": "Er9UqmEbUyNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokenizer.decode(output[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H2JBsAWKSpgT",
        "outputId": "e8555025-7519-45b2-8c35-1771e604904c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "IA en la logística optimiza términos de críticos posible Artificial destacadas cultural para ágil avanzados obstante operar operar Uno operar otros desarrollo avanzados soluciones efectiva\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMO VEMOS LA RESPUESTA ES BASTANTE MALA YA QUE EL VOCABULARIO ES MUY PEQUEÑO Y EL MODELO MUY SIMPLE. TIENDE A REPETIR PALABRAS YA QUE TAMPOCO ESTAMOS PENALIZANDO LA REPETICIÓN."
      ],
      "metadata": {
        "id": "NkDXMl0WYh74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **EJERCICIO 3-4**"
      ],
      "metadata": {
        "id": "3XrRoofRY7us"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "AHORA VAMOS A HACER LO MISMO PERO CON LA ESTRUCTURA DE UN GPT2-124M CON CONTEXTO REDUCIDO A 256 EN LUGAR DE 1028. TAMBIÉN SE HA REDUCIDO EL TAMAÑO DEL VOCABULARIO A 50257."
      ],
      "metadata": {
        "id": "psGAWb4Jdkxl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvJG5NG1bPZY",
        "outputId": "6e13ca74-def7-47b5-a510-3ab488bf0a81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.4.26)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\"matplotlib\",\n",
        "        \"numpy\",\n",
        "        \"tiktoken\",\n",
        "        \"torch\",\n",
        "       ]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30aVXb4pbQiO",
        "outputId": "bbf4900c-4790-4914-a97b-7e3564c9aff2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "matplotlib version: 3.10.0\n",
            "numpy version: 2.0.2\n",
            "tiktoken version: 0.9.0\n",
            "torch version: 2.6.0+cu124\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # Shape: (b, num_tokens, d_out)\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ],
      "metadata": {
        "id": "WQ1PtIOpbdu_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,   # Vocabulary size\n",
        "    \"context_length\": 256, # Shortened context length (orig: 1024)\n",
        "    \"emb_dim\": 768,        # Embedding dimension\n",
        "    \"n_heads\": 12,         # Number of attention heads\n",
        "    \"n_layers\": 12,        # Number of layers\n",
        "    \"drop_rate\": 0.1,      # Dropout rate\n",
        "    \"qkv_bias\": False      # Query-key-value bias\n",
        "}\n",
        "\n",
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.eval();  # Disable dropout during inference"
      ],
      "metadata": {
        "id": "yVYC1-ijbfDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "def text_to_token_ids(text, tokenizer):\n",
        "    encoded = tokenizer.encode(text, allowed_special={'<|endoftext|>'})\n",
        "    encoded_tensor = torch.tensor(encoded).unsqueeze(0) # add batch dimension\n",
        "    return encoded_tensor\n",
        "\n",
        "def token_ids_to_text(token_ids, tokenizer):\n",
        "    flat = token_ids.squeeze(0) # remove batch dimension\n",
        "    return tokenizer.decode(flat.tolist())"
      ],
      "metadata": {
        "id": "AWf6ZjxCbvgO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_context = 'IA en la logística optimiza'\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "token_ids = generate_text_simple(\n",
        "    model=model,\n",
        "    idx=text_to_token_ids(start_context, tokenizer),\n",
        "    max_new_tokens=10,\n",
        "    context_size=GPT_CONFIG_124M[\"context_length\"]\n",
        ")\n",
        "\n",
        "print(\"Output text:\\n\", token_ids_to_text(token_ids, tokenizer))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eiF60ajybydN",
        "outputId": "75ac4d14-10d7-4452-b67a-9fc9ade59ec7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output text:\n",
            " IA en la logística optimizaesh modifierificentatonfiguredlevision TTarianFree foundation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LA RESPUESTA SIGUE SIENDO BASTANTE MALA PORQUE EL MODELO NO HA SIDO ENTRENADO AÚN"
      ],
      "metadata": {
        "id": "rLHlZGqSmWR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJERCICIO 5"
      ],
      "metadata": {
        "id": "e7HzmClEaVru"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, txt, tokenizer, max_length, stride):\n",
        "        self.input_ids = []\n",
        "        self.target_ids = []\n",
        "\n",
        "        # Tokenize the entire text\n",
        "        token_ids = tokenizer.encode(txt, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "        # Use a sliding window to chunk the book into overlapping sequences of max_length\n",
        "        for i in range(0, len(token_ids) - max_length, stride):\n",
        "            input_chunk = token_ids[i:i + max_length]\n",
        "            target_chunk = token_ids[i + 1: i + max_length + 1]\n",
        "            self.input_ids.append(torch.tensor(input_chunk))\n",
        "            self.target_ids.append(torch.tensor(target_chunk))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.input_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.input_ids[idx], self.target_ids[idx]"
      ],
      "metadata": {
        "id": "tKQQS_Laba8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
        "                         stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    # Initialize the tokenizer\n",
        "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "    # Create dataset\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "\n",
        "    # Create dataloader\n",
        "    dataloader = DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=shuffle, drop_last=drop_last, num_workers=num_workers)\n",
        "\n",
        "    return dataloader"
      ],
      "metadata": {
        "id": "1wkoudPvbeYh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "GPT_CONFIG_124M"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RN7t155Nbgv5",
        "outputId": "af6bef2f-099a-49fd-c06d-bd217e838f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'vocab_size': 50257,\n",
              " 'context_length': 256,\n",
              " 'emb_dim': 768,\n",
              " 'n_heads': 12,\n",
              " 'n_layers': 12,\n",
              " 'drop_rate': 0.1,\n",
              " 'qkv_bias': False}"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En este caso hemos dejado el train_ratio en 0.80 ya que a 0.90 quedaba una validación muy pequeña que producía un NAN."
      ],
      "metadata": {
        "id": "J1vudZdNcPKq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train/validation ratio\n",
        "train_ratio = 0.80\n",
        "split_idx = int(train_ratio * len(raw_text))\n",
        "train_data = raw_text[:split_idx]\n",
        "val_data = raw_text[split_idx:]\n",
        "\n",
        "\n",
        "torch.manual_seed(123)\n",
        "\n",
        "train_loader = create_dataloader_v1(\n",
        "    train_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=True,\n",
        "    shuffle=True,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "val_loader = create_dataloader_v1(\n",
        "    val_data,\n",
        "    batch_size=2,\n",
        "    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
        "    stride=GPT_CONFIG_124M[\"context_length\"],\n",
        "    drop_last=False,\n",
        "    shuffle=False,\n",
        "    num_workers=0\n",
        ")"
      ],
      "metadata": {
        "id": "cifED1iwbnf7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calc_loss_batch(input_batch, target_batch, model, device):\n",
        "    input_batch, target_batch = input_batch.to(device), target_batch.to(device)\n",
        "    logits = model(input_batch)\n",
        "    loss = torch.nn.functional.cross_entropy(logits.flatten(0, 1), target_batch.flatten())\n",
        "    return loss\n",
        "\n",
        "\n",
        "def calc_loss_loader(data_loader, model, device, num_batches=None):\n",
        "    total_loss = 0.\n",
        "    if len(data_loader) == 0:\n",
        "        return float(\"nan\")\n",
        "    elif num_batches is None:\n",
        "        num_batches = len(data_loader)\n",
        "    else:\n",
        "        # Reduce the number of batches to match the total number of batches in the data loader\n",
        "        # if num_batches exceeds the number of batches in the data loader\n",
        "        num_batches = min(num_batches, len(data_loader))\n",
        "    for i, (input_batch, target_batch) in enumerate(data_loader):\n",
        "        if i < num_batches:\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            total_loss += loss.item()\n",
        "        else:\n",
        "            break\n",
        "    return total_loss / num_batches"
      ],
      "metadata": {
        "id": "-_6Q2tkLcFui"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device) # no assignment model = model.to(device) necessary for nn.Module classes\n",
        "\n",
        "\n",
        "torch.manual_seed(123) # For reproducibility due to the shuffling in the data loader\n",
        "\n",
        "with torch.no_grad(): # Disable gradient tracking for efficiency because we are not training, yet\n",
        "    train_loss = calc_loss_loader(train_loader, model, device)\n",
        "    val_loss = calc_loss_loader(val_loader, model, device)\n",
        "\n",
        "print(\"Training loss:\", train_loss)\n",
        "print(\"Validation loss:\", val_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unwGUQfUcK0D",
        "outputId": "079dc4e5-86ff-4460-e9af-519153bc510a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 10.980511665344238\n",
            "Validation loss: 11.019054412841797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def evaluate_model(model, train_loader, val_loader, device, eval_iter):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        train_loss = calc_loss_loader(train_loader, model, device, num_batches=eval_iter)\n",
        "        val_loss = calc_loss_loader(val_loader, model, device, num_batches=eval_iter)\n",
        "    model.train()\n",
        "    return train_loss, val_loss\n",
        "\n",
        "def generate_and_print_sample(model, tokenizer, device, start_context):\n",
        "    model.eval()\n",
        "    context_size = model.pos_emb.weight.shape[0]\n",
        "    encoded = text_to_token_ids(start_context, tokenizer).to(device)\n",
        "    with torch.no_grad():\n",
        "        token_ids = generate_text_simple(\n",
        "            model=model, idx=encoded,\n",
        "            max_new_tokens=50, context_size=context_size\n",
        "        )\n",
        "        decoded_text = token_ids_to_text(token_ids, tokenizer)\n",
        "        print(decoded_text.replace(\"\\n\", \" \"))  # Compact print format\n",
        "    model.train()"
      ],
      "metadata": {
        "id": "ryuN4kcjdg2G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model_simple(model, train_loader, val_loader, optimizer, device, num_epochs,\n",
        "                       eval_freq, eval_iter, start_context, tokenizer):\n",
        "    # Initialize lists to track losses and tokens seen\n",
        "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # Main training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()  # Set model to training mode\n",
        "\n",
        "        for input_batch, target_batch in train_loader:\n",
        "            optimizer.zero_grad() # Reset loss gradients from previous batch iteration\n",
        "            loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "            loss.backward() # Calculate loss gradients\n",
        "            optimizer.step() # Update model weights using loss gradients\n",
        "            tokens_seen += input_batch.numel()\n",
        "            global_step += 1\n",
        "\n",
        "            # Optional evaluation step\n",
        "            if global_step % eval_freq == 0:\n",
        "                train_loss, val_loss = evaluate_model(\n",
        "                    model, train_loader, val_loader, device, eval_iter)\n",
        "                train_losses.append(train_loss)\n",
        "                val_losses.append(val_loss)\n",
        "                track_tokens_seen.append(tokens_seen)\n",
        "                print(f\"Ep {epoch+1} (Step {global_step:06d}): \"\n",
        "                      f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
        "\n",
        "        # Print a sample text after each epoch\n",
        "        generate_and_print_sample(\n",
        "            model, tokenizer, device, start_context\n",
        "        )\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen"
      ],
      "metadata": {
        "id": "whGb7e50dlHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)\n",
        "model.to(device)\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0004, weight_decay=0.1)\n",
        "\n",
        "num_epochs = 10\n",
        "train_losses, val_losses, tokens_seen = train_model_simple(\n",
        "    model, train_loader, val_loader, optimizer, device,\n",
        "    num_epochs=num_epochs, eval_freq=5, eval_iter=5,\n",
        "    start_context=\"IA en la logística optimiza\", tokenizer=tokenizer\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mmYq-SndqW4",
        "outputId": "9b15be24-9e4a-4b7d-80d9-0e9303b7df91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ep 1 (Step 000000): Train loss 9.572, Val loss 9.670\n",
            "IA en la logística optimiza de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de de\n",
            "IA en la logística optimiza, la IA, la IA en la IA la IA en la IA, la de la IA la IA en la IA de la IA de la IA en la IA, la IA de la IA en la IA, la IA en la IAes de\n",
            "Ep 3 (Step 000005): Train loss 4.930, Val loss 7.826\n",
            "IA en la logística optimiza el de la IA en la IA en la IA en la IA de la IA en la IA en la IA en la IA en la IA de la IA en la IA en la IA en la IA en la IA de la IA de la IA de\n",
            "IA en la logística optimizaos, la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en la IA en\n",
            "IA en la logística optimizaas. Esto de rutas, la IA en la IA en el de la IA en la IA en la IA en la IA en la IA en la IA en tiempo real de la IA en la IA en la IAes de\n",
            "Ep 6 (Step 000010): Train loss 3.190, Val loss 6.389\n",
            "IA en la logística optimiza el ámbito logística la IA en la IA en tiempo real y el áos.  Resulta y el á, la IA en la IA en la IA en tiempo real. Esto\n",
            "IA en la logística optimiza el, de los serviccia artificial redefine la IA permitenibilidad en el transporte de las aplicaciones de de los real para predecir con precisión de las aplicaciones y unaes de\n",
            "Ep 8 (Step 000015): Train loss 1.595, Val loss 5.589\n",
            "IA en la logística optimiza el ámbito logística la IA permitenibilidad en el transporte de la IA en tiempo real de la IA en tiempo realización de la IA en tiempo real. Esto\n",
            "IA en la logística optimiza el, los algoritmos basados en tiempo realizar tanto datos de las aplicos. Esto de estar comprometidos con la adopción y el desarrollo de soluc\n",
            "IA en la logística optimiza el, los algoritmos basados en IA permitenibilidad en el transporte de las aplicaciones de los costos. Estas. Esto permite planificar la IA en tiempo, lo\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COMO VEMOS LA SOLUCIÓN MEJORA NOTABLEMENTE A MEDIDA QUE PASAN LAS EPOCHS, HEMOS PODIDO REALIZAR EL ENTRENAMIENTO YA QUE EL TEXTO ES BREVE. PERO DADO EL TAMAÑO DE NUESTRO TEXTO LA SOLUCIÓN NO ES DEL TODO BUENA."
      ],
      "metadata": {
        "id": "3B_MpehphJCp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJERCICIO 6"
      ],
      "metadata": {
        "id": "3dUtbJnJgHkx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "model_name = \"deepset/roberta-base-squad2\"\n",
        "\n",
        "# Crear un pipeline de question answering\n",
        "qa_pipeline = pipeline(\"question-answering\", model=model_name)\n",
        "\n",
        "# Contexto y pregunta\n",
        "\n",
        "contexto = \"\"\"\n",
        "PSG (Paris Saint-Germain) goleó 5-0 a Inter la gran final y se consagró campeón de la UEFA Champions League 2025, en el Allianz Arena de Múnich en una exhibición de fútbol para el recuerdo que significó la goleada más abultada en un partido de este calibre.\n",
        "El equipo de Luis Enrique obtuvo su primera corona en Europa gracias a los goles de Achraf Hakimi, Désiré Doué (en dos ocasiones), Khvicha Kvaratskhelia y Senny Mayulu en una demostración futbolística que sin lugar a dudas quedará en los libros de historia.\n",
        "\"\"\"\n",
        "\n",
        "pregunta = \"¿Quién ganó la Champions League 2025?\"\n",
        "\n",
        "# Realizar la tarea de question answering\n",
        "respuesta = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {respuesta['answer']}\")\n",
        "print(f\"Puntuación: {respuesta['score']:.4f}\")\n",
        "print(f\"Inicio: {respuesta['start']}\")\n",
        "print(f\"Fin: {respuesta['end']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CgPye0BQgMBL",
        "outputId": "8474d376-82b5-4a7b-ee45-c9a02822dc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: ¿Quién ganó la Champions League 2025?\n",
            "Respuesta: Paris Saint-Germain\n",
            "Puntuación: 0.0495\n",
            "Inicio: 6\n",
            "Fin: 25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Crear un pipeline de question answering\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"MMG/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es-finetuned-sqac\")\n",
        "\n",
        "# Contexto y pregunta\n",
        "contexto = \"\"\"\n",
        " PSG (Paris Saint-Germain) goleó 5-0 a Inter la gran final y se consagró campeón de la UEFA Champions League 2025, en el Allianz Arena de Múnich en una exhibición de fútbol para el recuerdo que significó la goleada más abultada en un partido de este calibre.\n",
        "El equipo de Luis Enrique obtuvo su primera corona en Europa gracias a los goles de Achraf Hakimi, Désiré Doué (en dos ocasiones), Khvicha Kvaratskhelia y Senny Mayulu en una demostración futbolística que sin lugar a dudas quedará en los libros de historia.\n",
        "\"\"\"\n",
        "\n",
        "pregunta = \"¿Quién ganó la Champions League 2025?\"\n",
        "\n",
        "# Realizar la tarea de question answering\n",
        "respuesta = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {respuesta['answer']}\")\n",
        "print(f\"Puntuación: {respuesta['score']:.4f}\")\n",
        "print(f\"Inicio: {respuesta['start']}\")\n",
        "print(f\"Fin: {respuesta['end']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oyx9Yu0-i2ZD",
        "outputId": "c94b631e-4125-4338-e08f-224d5c78f0f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: ¿Quién ganó la Champions League 2025?\n",
            "Respuesta: PSG\n",
            "Puntuación: 0.8419\n",
            "Inicio: 2\n",
            "Fin: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Crear un pipeline de question answering\n",
        "qa_pipeline = pipeline(\"question-answering\", model=\"MMG/bert-base-spanish-wwm-cased-finetuned-spa-squad2-es-finetuned-sqac\")\n",
        "\n",
        "# Contexto y pregunta\n",
        "contexto = \"\"\"\n",
        " PSG (Paris Saint-Germain) goleó 5-0 a Inter la gran final y se consagró campeón de la UEFA Champions League 2025, en el Allianz Arena de Múnich en una exhibición de fútbol para el recuerdo que significó la goleada más abultada en un partido de este calibre.\n",
        "El equipo de Luis Enrique obtuvo su primera corona en Europa gracias a los goles de Achraf Hakimi, Désiré Doué (en dos ocasiones), Khvicha Kvaratskhelia y Senny Mayulu en una demostración futbolística que sin lugar a dudas quedará en los libros de historia.\n",
        "\"\"\"\n",
        "\n",
        "pregunta = \"¿Cual fue el resultado de la final de la Champions 2025?\"\n",
        "# Realizar la tarea de question answering\n",
        "respuesta = qa_pipeline(question=pregunta, context=contexto)\n",
        "\n",
        "# Mostrar los resultados\n",
        "print(f\"Pregunta: {pregunta}\")\n",
        "print(f\"Respuesta: {respuesta['answer']}\")\n",
        "print(f\"Puntuación: {respuesta['score']:.4f}\")\n",
        "print(f\"Inicio: {respuesta['start']}\")\n",
        "print(f\"Fin: {respuesta['end']}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sa6sFR9Hkw-A",
        "outputId": "c84a2b00-8954-4e05-a520-501348920bfd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pregunta: ¿Cual fue el resultado de la final de la Champions 2025?\n",
            "Respuesta: 5-0\n",
            "Puntuación: 0.3084\n",
            "Inicio: 34\n",
            "Fin: 37\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Como se observa ambos modelos han dado la respuesta correcta. Si bien es verdad que el primero dado que el idioma es el inglés el score que tiene, es decir la seguridad en la respuesta es muy baja. En cambio en el modelo utilizado como ejemplo en clase que tiene un corpus en español el score es mucho más alto."
      ],
      "metadata": {
        "id": "h6dSJN3SjIsb"
      }
    }
  ]
}