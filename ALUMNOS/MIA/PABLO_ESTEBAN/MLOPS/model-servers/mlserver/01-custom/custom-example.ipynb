{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model servers: MLServer custom example"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Write the models.py file\n",
    "\n",
    "This file will contain the logic of your model. It must contain a class with the following methods:\n",
    "- load: Loads the model into memory\n",
    "- predict: Makes a prediction with the model\n",
    "\n",
    "The class can have any other methods or attributes you need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile models.py\n",
    "\n",
    "from mlserver import MLModel\n",
    "from mlserver.codecs import decode_args\n",
    "from mlserver.utils import get_model_uri\n",
    "from mlserver.errors import InferenceError\n",
    "from mlserver.types import InferenceRequest, InferenceResponse\n",
    "from mlserver import types\n",
    "from pydantic import BaseModel\n",
    "from typing import List\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "\n",
    "class customModel(MLModel):\n",
    "    async def load(self) -> bool:\n",
    "        \n",
    "        model_uri = await get_model_uri(self._settings) #\"../../model.pkl\" #\n",
    "        \n",
    "        with open(model_uri, 'rb') as f:\n",
    "            self._model = joblib.load(f)\n",
    "            \n",
    "        self.ready = True\n",
    "        return self.ready\n",
    "    \n",
    "    @decode_args\n",
    "    async def predict(self, data: np.ndarray) -> np.ndarray:\n",
    "        data = data.reshape(1, -1)\n",
    "        data = data.astype(np.float32)\n",
    "        \n",
    "        predictions = self._model.predict(data)\n",
    "        return np.asarray(predictions)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Write the settings and model-settings files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile settings.json\n",
    "{\n",
    "    \"debug\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile model-settings.json\n",
    "{\n",
    "    \"name\": \"iris-rf-custom\",\n",
    "    \"implementation\": \"models.customModel\",\n",
    "    \"parameters\": {\n",
    "        \"uri\": \"../model.pkl\",\n",
    "        \"version\": \"v0.1.0\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start the model server\n",
    "\n",
    "If running in colab:\n",
    "    \n",
    "    ```bash\n",
    "    ! nohup mlserver start . &\n",
    "    ```\n",
    "\n",
    "If running locally:\n",
    "\n",
    "    ```bash\n",
    "    mlserver start .\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the process is running\n",
    "! ps | grep mlserver"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Make a prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from mlserver.types import InferenceRequest\n",
    "from mlserver.codecs import NumpyCodec\n",
    "\n",
    "x_0 = np.asarray([[5.9, 3. , 5.1, 1.8]])\n",
    "inference_request = InferenceRequest(\n",
    "    inputs=[\n",
    "        NumpyCodec.encode_input(name=\"data\", payload=x_0)\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "endpoint = \"http://localhost:8080/v2/models/iris-rf-custom/versions/v0.1.0/infer\"\n",
    "response = requests.post(endpoint, json=inference_request.dict())\n",
    "\n",
    "response.json()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-servers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
