{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd",
   "metadata": {
    "id": "d95f841a-63c9-41d4-aea1-496b3d2024dd"
   },
   "source": [
    "**LLM Workshop 2024 by Sebastian Raschka**\n",
    "\n",
    "This code is based on *Build a Large Language Model (From Scratch)*, [https://github.com/rasbt/LLMs-from-scratch](https://github.com/rasbt/LLMs-from-scratch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25aa40e3-5109-433f-9153-f5770531fe94",
   "metadata": {
    "id": "25aa40e3-5109-433f-9153-f5770531fe94"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2) Understanding LLM Input Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf",
   "metadata": {
    "id": "76d5d2c0-cba8-404e-9bf3-71a218cae3cf"
   },
   "source": [
    "Packages that are being used in this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8861a653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (2.6.0)\n",
      "Requirement already satisfied: tiktoken in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (0.9.0)\n",
      "Requirement already satisfied: filelock in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: networkx in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (2024.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (75.2.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from tiktoken) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from tiktoken) (2.32.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install torch tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
   "metadata": {
    "id": "4d1305cf-12d5-46fe-a2c9-36fb71c5b3d3",
    "outputId": "e4a75ba8-b2ad-40a2-f8a6-02eaf8fd7231"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch version: 2.6.0\n",
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "\n",
    "print(\"torch version:\", version(\"torch\"))\n",
    "print(\"tiktoken version:\", version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a",
   "metadata": {
    "id": "5a42fbfd-e3c2-43c2-bc12-f5f870a0b10a"
   },
   "source": [
    "- This notebook provides a brief overview of the data preparation and sampling procedures to get input data \"ready\" for an LLM\n",
    "- Understanding what the input data looks like is a great first step towards understanding how LLMs work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5",
   "metadata": {
    "id": "628b2922-594d-4ff9-bd82-04f1ebdf41f5"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/590a463dcb825375473c9fd366013e86204589d68be0bd0207d43b158ba10558/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30312e776562703f74696d657374616d703d31\" width=\"700px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3",
   "metadata": {
    "id": "eddbb984-8d23-40c5-bbfa-c3c379e7eec3"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.1 Tokenizing text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80",
   "metadata": {
    "id": "f9c90731-7dc9-4cd3-8c4a-488e33b48e80"
   },
   "source": [
    "- In this section, we tokenize text, which means breaking text into smaller units, such as individual words and punctuation characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b",
   "metadata": {
    "id": "09872fdb-9d4e-40c4-949d-52a01a43ec4b"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/b92bf8c18c5d51258b4a8a55d9612fd1a2eb5f3b6c6a79fc0a1d7b1ba59a2e99/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30342e77656270\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XalQquvs1uBy",
   "metadata": {
    "id": "XalQquvs1uBy"
   },
   "source": [
    "Enlace a texto: https://drive.google.com/file/d/1H_ZU_35t3sqg9LklLau-twiWKNQraonx/view?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cceaa18-833d-46b6-b211-b20c53902805",
   "metadata": {
    "id": "8cceaa18-833d-46b6-b211-b20c53902805"
   },
   "source": [
    "- Load raw text we want to work with\n",
    "- [The Verdict by Edith Wharton](https://en.wikisource.org/wiki/The_Verdict) is a public domain short story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a769e87-470a-48b9-8bdb-12841b416198",
   "metadata": {
    "id": "8a769e87-470a-48b9-8bdb-12841b416198",
    "outputId": "1cbd4963-8fcd-44d0-fca1-96bb96b375d4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of character: 20479\n",
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no \n"
     ]
    }
   ],
   "source": [
    "with open(\"the-verdict.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    raw_text = f.read()\n",
    "\n",
    "print(\"Total number of character:\", len(raw_text))\n",
    "print(raw_text[:99])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e",
   "metadata": {
    "id": "9b971a46-ac03-4368-88ae-3f20279e8f4e"
   },
   "source": [
    "- The goal is to tokenize and embed this text for an LLM\n",
    "- Let's develop a simple tokenizer based on some simple sample text that we can then later apply to the text above"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a",
   "metadata": {
    "id": "6cbe9330-b587-4262-be9f-497a84ec0e8a"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/241f7a302c33bc1e8156e7d0b153caae8728f2c9cd03884487c05d931fd88be2/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30352e77656270\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3daa1687-2c08-485a-87cc-a93c2f9586d7",
   "metadata": {
    "id": "3daa1687-2c08-485a-87cc-a93c2f9586d7"
   },
   "source": [
    "- The following regular expression will split on whitespaces and punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3588a341",
   "metadata": {
    "id": "3588a341"
   },
   "source": [
    "#### Ejercicio\n",
    "\n",
    "Crea el c√≥digo que permita dividir el texto siempre que haya uno de los siguientes caracteres empleando expresiones regulares:\n",
    "\n",
    "- ,\n",
    "- .\n",
    "- :\n",
    "- ;\n",
    "- ?\n",
    "- _\n",
    "- !\n",
    "- \"\n",
    "- (\n",
    "- )\n",
    "- '\n",
    "\n",
    "o bien:\n",
    "\n",
    "- \"--\"\n",
    "\n",
    "o bien:\n",
    "\n",
    "- \" \"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7",
   "metadata": {
    "id": "737dd5b0-9dbb-4a97-9ae4-3482c8c04be7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', ' ', 'HAD', ' ', 'always', ' ', 'thought', ' ', 'Jack', ' ', 'Gisburn', ' ', 'rather', ' ', 'a', ' ', 'cheap', ' ', 'genius', '--', 'though', ' ', 'a', ' ', 'good', ' ', 'fellow', ' ', 'enough', '--', 'so', ' ', 'it', ' ', 'was', ' ', 'no', ' ']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "preprocessed = re.split(r'([,.:;?_!\"()\\']|--|\\s)', raw_text)\n",
    "preprocessed = [item for item in preprocessed if item]\n",
    "print(preprocessed[:38])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c",
   "metadata": {
    "id": "35db7b5e-510b-4c45-995f-f5ad64a8e19c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tokens: 8405\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of tokens:\", len(preprocessed))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231",
   "metadata": {
    "id": "0b5ce8fe-3a07-4f2a-90f1-a0321ce3a231"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.2 Converting tokens into token IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff",
   "metadata": {
    "id": "a5204973-f414-4c0d-87b0-cfec1f06e6ff"
   },
   "source": [
    "- Next, we convert the text tokens into token IDs that we can process via embedding layers later\n",
    "- For this we first need to build a vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d",
   "metadata": {
    "id": "177b041d-f739-43b8-bd81-0443ae3a7f8d"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/bf01ba4b1b924633325cda845feac84ae1a3f154db5098f5d70e90470ff4484e/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30362e77656270\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eeade64-037b-4b59-9039-d3b000ef8886",
   "metadata": {
    "id": "8eeade64-037b-4b59-9039-d3b000ef8886"
   },
   "source": [
    "- The vocabulary contains the unique words in the input text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
   "metadata": {
    "id": "7fdf0533-5ab6-42a5-83fa-a3b045de6396",
    "outputId": "10c4521a-60f1-419a-9f77-efacfff5e06e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1132\n"
     ]
    }
   ],
   "source": [
    "all_words = sorted(set(preprocessed))\n",
    "vocab_size = len(all_words)\n",
    "\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b636d953",
   "metadata": {
    "id": "b636d953"
   },
   "source": [
    "#### Ejercicio\n",
    "\n",
    "A continuaci√≥n deber√©is crear el vocabulario. La forma m√°s sencilla es emplear `all_words` para crear un diccinario en la forma: `{palabra1: 0, palabra2: 1, ...}`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "77d00d96-881f-4691-bb03-84fec2a75a26",
   "metadata": {
    "id": "77d00d96-881f-4691-bb03-84fec2a75a26"
   },
   "outputs": [],
   "source": [
    "vocab = {token:integer for integer,token in enumerate(all_words)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3",
   "metadata": {
    "id": "75bd1f81-3a8f-4dd9-9dd6-e75f32dacbe3"
   },
   "source": [
    "- Below are the first 50 entries in this vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6",
   "metadata": {
    "id": "e1c5de4a-aa4e-4aec-b532-10bb364039d6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\\n', 0)\n",
      "(' ', 1)\n",
      "('!', 2)\n",
      "('\"', 3)\n",
      "(\"'\", 4)\n",
      "('(', 5)\n",
      "(')', 6)\n",
      "(',', 7)\n",
      "('--', 8)\n",
      "('.', 9)\n",
      "(':', 10)\n",
      "(';', 11)\n",
      "('?', 12)\n",
      "('A', 13)\n",
      "('Ah', 14)\n",
      "('Among', 15)\n",
      "('And', 16)\n",
      "('Are', 17)\n",
      "('Arrt', 18)\n",
      "('As', 19)\n",
      "('At', 20)\n",
      "('Be', 21)\n",
      "('Begin', 22)\n",
      "('Burlington', 23)\n",
      "('But', 24)\n",
      "('By', 25)\n",
      "('Carlo', 26)\n",
      "('Chicago', 27)\n",
      "('Claude', 28)\n",
      "('Come', 29)\n",
      "('Croft', 30)\n",
      "('Destroyed', 31)\n",
      "('Devonshire', 32)\n",
      "('Don', 33)\n",
      "('Dubarry', 34)\n",
      "('Emperors', 35)\n",
      "('Florence', 36)\n",
      "('For', 37)\n",
      "('Gallery', 38)\n",
      "('Gideon', 39)\n",
      "('Gisburn', 40)\n",
      "('Gisburns', 41)\n",
      "('Grafton', 42)\n",
      "('Greek', 43)\n",
      "('Grindle', 44)\n",
      "('Grindles', 45)\n",
      "('HAD', 46)\n",
      "('Had', 47)\n",
      "('Hang', 48)\n",
      "('Has', 49)\n",
      "('He', 50)\n"
     ]
    }
   ],
   "source": [
    "for i, item in enumerate(vocab.items()):\n",
    "    print(item)\n",
    "    if i >= 50:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19",
   "metadata": {
    "id": "3b1dc314-351b-476a-9459-0ec9ddc29b19"
   },
   "source": [
    "- Below, we illustrate the tokenization of a short sample text using a small vocabulary:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc",
   "metadata": {
    "id": "67407a9f-0202-4e7c-9ed7-1b3154191ebc"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/8955d3aea45dc06f156d0579f7f3302c27b6635e649c301dbab33427b2d8d2a8/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30372e776562703f313233\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a",
   "metadata": {
    "id": "4e569647-2589-4c9d-9a5c-aef1c88a0a9a"
   },
   "source": [
    "- Let's now put it all together into a tokenizer class"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff243e4",
   "metadata": {
    "id": "1ff243e4"
   },
   "source": [
    "C√≥mo podr√©is imaginaros, en la vida real esto no se hace con un jupyter notebook y con funciones, sino que se programan clases que puedan organizar y modularizar el c√≥digo para poder reaprovecharlo.\n",
    "\n",
    "A continuaci√≥n crear√©is una clase llamada `SimpleTokenizerV1` que tendr√° un m√©todo `__init__(vocab)` que permitir√° iniciar el vocabulario para poder convertir de caracter a ID (int) y de ID (int) a caracter.\n",
    "\n",
    "Adem√°s, tendr√° tambi√©n un m√©todo `encode(self, text)` que se encargar√° de devolver las IDs del texto que tiene como entrada. La implementaci√≥n de este m√©todo es muy sencilla si os bas√°is en lo hecho previamente.\n",
    "\n",
    "Por √∫ltimo, habr√° un m√©todo `decode(self, ids)` que permitir√° convertir de IDs a texto. En este m√©todo tendr√©is que emplear la variable `self.int_to_str` para pasar de IDs a caracteres, y luego concatenar todos los caracteres para obtener una cadena de texto.\n",
    "\n",
    "La √∫ltima l√≠nea, `text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)`, se encarga de eliminar los espacios **previos** a los s√≠mbolos indicados en la primera cadena del `re.sub(...)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5",
   "metadata": {
    "id": "f531bf46-7c25-4ef8-bff8-0d27518676d5"
   },
   "outputs": [],
   "source": [
    "class SimpleTokenizerV1:\n",
    "    def __init__(self, vocab):\n",
    "        self.str_to_int = vocab\n",
    "        self.int_to_str = {i:s for s,i in vocab.items()}\n",
    "\n",
    "    def encode(self, text):\n",
    "        preprocessed = re.split(r'([,.?_!\"()\\']|--|\\s)', text)\n",
    "        preprocessed = [\n",
    "            item.strip() for item in preprocessed if item.strip()\n",
    "        ]\n",
    "        ids = [self.str_to_int[s] for s in preprocessed]\n",
    "        return ids\n",
    "\n",
    "    def decode(self, ids):\n",
    "        text = \" \".join([self.int_to_str[i] for i in ids])\n",
    "        # Replace spaces before the specified punctuations\n",
    "        text = re.sub(r'\\s+([,.?!\"()\\'])', r'\\1', text)\n",
    "        return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7",
   "metadata": {
    "id": "dee7a1e5-b54f-4ca1-87ef-3d663c4ee1e7"
   },
   "source": [
    "- The `encode` function turns text into token IDs\n",
    "- The `decode` function turns token IDs back into text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc21d347-ec03-4823-b3d4-9d686e495617",
   "metadata": {
    "id": "cc21d347-ec03-4823-b3d4-9d686e495617"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/b324e29fe9d3d4191a9200d6a08983eef4d3f835cff85ce1ee4aceb47117891a/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f30382e776562703f313233\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c",
   "metadata": {
    "id": "c2950a94-6b0d-474e-8ed0-66d0c3c1a95c"
   },
   "source": [
    "- We can use the tokenizer to encode (that is, tokenize) texts into integers\n",
    "- These integers can then be embedded (later) as input of/for the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4",
   "metadata": {
    "id": "647364ec-7995-4654-9b4a-7607ccf5f1e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 58, 4, 852, 990, 604, 535, 748, 7, 1128, 598, 7, 3, 69, 9, 40, 853, 1110, 756, 795, 9]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SimpleTokenizerV1(vocab)\n",
    "\n",
    "text = \"\"\"\"It's the last he painted, you know,\"\n",
    "           Mrs. Gisburn said with pardonable pride.\"\"\"\n",
    "ids = tokenizer.encode(text)\n",
    "print(ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3201706e-a487-4b60-b99d-5765865f29a0",
   "metadata": {
    "id": "3201706e-a487-4b60-b99d-5765865f29a0"
   },
   "source": [
    "- We can decode the integers back into text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "01d8c8fb-432d-4a49-b332-99f23b233746",
   "metadata": {
    "id": "01d8c8fb-432d-4a49-b332-99f23b233746"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "54f6aa8b-9827-412e-9035-e827296ab0fe",
   "metadata": {
    "id": "54f6aa8b-9827-412e-9035-e827296ab0fe"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\" It\\' s the last he painted, you know,\" Mrs. Gisburn said with pardonable pride.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tokenizer.encode(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "002745fd",
   "metadata": {
    "id": "002745fd"
   },
   "source": [
    "Si todo ha ido bien, estas dos √∫ltimas celdas deber√≠an devolver la misma cadena que hay disponible en la variable `text`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4ba34b-170f-4e71-939b-77aabb776f14",
   "metadata": {
    "id": "5c4ba34b-170f-4e71-939b-77aabb776f14"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.3 BytePair encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3926e2",
   "metadata": {
    "id": "ee3926e2"
   },
   "source": [
    "Byte-Pair Encoding (BPE) es una t√©cnica que permite codificar el contenido a nivel de sub-palabra. Su m√©todo b√°sico de funcionamiento es:\n",
    "\n",
    "1. Se parte de un vocabulario con todos los caracteres individuales\n",
    "2. Se van incorporando nuevos pares de tokens bas√°ndose en su mayor frecuencia de aparici√≥n en el texto a tokenizar\n",
    "3. As√≠ hasta llegar a un tama√±o de vocabulario definido (hiperpar√°metro)\n",
    "\n",
    "Por ejemplo:\n",
    "\n",
    "1. Partimos del vocabulario: `u, g, n`\n",
    "2. Se crean los siguientes pares:\n",
    "\n",
    "```\n",
    "(\"u\", \"g\") -> \"ug\"\n",
    "(\"u\", \"n\") -> \"un\"\n",
    "(\"h\", \"ug\") -> \"hug\"\n",
    "```\n",
    "\n",
    "La palabra \"bug\" ser√° tokenizada como [\"b\", \"ug\"]. En cambio, \"mug\", ser√° tokenizado como [\"[UNK]\", \"ug\"] dado que la letra \"m\" no fue parte del vocabulario base. De la misma manera, la palabra \"thug\" ser√° tokenizada como [\"[UNK]\", \"hug\"]: la letra \"t\" no est√° en el vocabulario base, y aplicando las reglas de fusi√≥n resulta primero la fusi√≥n de \"u\" y \"g\" y luego de \"hu\" and \"g\".\n",
    "\n",
    "Se trata de un m√©todo de tokenizaci√≥n mucho m√°s sofisticado que proporciona mayor velocidad. De hecho, es el tokenizador empleado para entrenar GPT-2, GPT-3, y ChatGPT, entre otros.\n",
    "\n",
    "Aqu√≠ pod√©is ver un tutorial en el que explican detalladamente su funcionamiento: https://huggingface.co/learn/nlp-course/es/chapter6/5.\n",
    "\n",
    "Y aqu√≠ otro de Sebastian Raschka en el que explica su implementaci√≥n paso a paso: https://sebastianraschka.com/blog/2025/bpe-from-scratch.html.\n",
    "\n",
    "Como su implementaci√≥n puede resultar complicada, vamos a emplear una librer√≠a open-source disponible en Python: `tiktoken` (https://github.com/openai/tiktoken), que implementa el algoritmo BPE de forma muy eficiente en Rust.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2309494c-79cf-4a2d-bc28-a94d602f050e",
   "metadata": {
    "id": "2309494c-79cf-4a2d-bc28-a94d602f050e"
   },
   "source": [
    "More info:\n",
    "- GPT-2 used BytePair encoding (BPE) as its tokenizer\n",
    "- it allows the model to break down words that aren't in its predefined vocabulary into smaller subword units or even individual characters, enabling it to handle out-of-vocabulary words\n",
    "- For instance, if GPT-2's vocabulary doesn't have the word \"unfamiliarword,\" it might tokenize it as [\"unfam\", \"iliar\", \"word\"] or some other subword breakdown, depending on its trained BPE merges\n",
    "- The original BPE tokenizer can be found here: [https://github.com/openai/gpt-2/blob/master/src/encoder.py](https://github.com/openai/gpt-2/blob/master/src/encoder.py)\n",
    "- In this lecture, we are using the BPE tokenizer from OpenAI's open-source [tiktoken](https://github.com/openai/tiktoken) library, which implements its core algorithms in Rust to improve computational performance\n",
    "- (Based on an analysis [here](https://github.com/rasbt/LLMs-from-scratch/blob/main/ch02/02_bonus_bytepair-encoder/compare-bpe-tiktoken.ipynb), I found that `tiktoken` is approx. 3x faster than the original tokenizer and 6x faster than an equivalent tokenizer in Hugging Face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ede1d41f-934b-4bf4-8184-54394a257a94",
   "metadata": {
    "id": "ede1d41f-934b-4bf4-8184-54394a257a94"
   },
   "outputs": [],
   "source": [
    "# pip install tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "48967a77-7d17-42bf-9e92-fc619d63a59e",
   "metadata": {
    "id": "48967a77-7d17-42bf-9e92-fc619d63a59e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tiktoken version: 0.9.0\n"
     ]
    }
   ],
   "source": [
    "import importlib\n",
    "import tiktoken\n",
    "\n",
    "print(\"tiktoken version:\", importlib.metadata.version(\"tiktoken\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128",
   "metadata": {
    "id": "6ad3312f-a5f7-4efc-9d7d-8ea09d7b5128"
   },
   "outputs": [],
   "source": [
    "tokenizer = tiktoken.get_encoding(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5ff2cd85-7cfb-4325-b390-219938589428",
   "metadata": {
    "id": "5ff2cd85-7cfb-4325-b390-219938589428"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15496, 11, 466, 345, 588, 8887, 30, 220, 50256, 554, 262, 4252, 18250, 8812, 2114, 1659, 617, 34680, 27271, 13]\n"
     ]
    }
   ],
   "source": [
    "text = (\n",
    "    \"Hello, do you like tea? <|endoftext|> In the sunlit terraces\"\n",
    "     \"of someunknownPlace.\"\n",
    ")\n",
    "\n",
    "integers = tokenizer.encode(text, allowed_special={\"<|endoftext|>\"})\n",
    "\n",
    "print(integers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab",
   "metadata": {
    "id": "d26a48bb-f82e-41a8-a955-a1c9cf9d50ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, do you like tea? <|endoftext|> In the sunlit terracesof someunknownPlace.\n"
     ]
    }
   ],
   "source": [
    "strings = tokenizer.decode(integers)\n",
    "\n",
    "print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a",
   "metadata": {
    "id": "e8c2e7b4-6a22-42aa-8e4d-901f06378d4a"
   },
   "source": [
    "- BPE tokenizers break down unknown words into subwords and individual characters:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c",
   "metadata": {
    "id": "c082d41f-33d7-4827-97d8-993d5a84bb3c"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/5938dff392e5cb7404d2636e4d7157fceb4c36ecf57a2173001bd3edf22234da/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31312e77656270\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0beb27ee-1156-457c-839e-eebb48d94d0e",
   "metadata": {
    "id": "0beb27ee-1156-457c-839e-eebb48d94d0e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[33901, 86, 343, 86, 220, 959]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode(\"Akwirw ier\", allowed_special={\"<|endoftext|>\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "634a8208",
   "metadata": {
    "id": "634a8208"
   },
   "source": [
    "This code allows the special token \"*<|endoftext|>*\" to be encoded as a special token if it appears in the input text. However, since \"Akwirw ier\" doesn't contain \"<|endoftext|>\", the allowed_special parameter won't have any effect on the encoding of this specific input.\n",
    "\n",
    "The `allowed_special` parameter is particularly useful when you want to include certain special tokens in your input without raising errors or having them split into regular tokens.\n",
    "\n",
    "It's a safety measure to prevent accidental encoding of special tokens that might have unintended effects on model behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abbd7c0d-70f8-4386-a114-907e96c950b0",
   "metadata": {
    "id": "abbd7c0d-70f8-4386-a114-907e96c950b0"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# 2.4 Data sampling with a sliding window"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec9958f7",
   "metadata": {
    "id": "ec9958f7"
   },
   "source": [
    "Por √∫ltimo, vamos a abordar el formato de los datos de entrenamiento. En el caso de los modelos GPT (Generative Pre-Trained models), el modelo va a aprender a predecir la siguiente palabra dada una secuencia de entrada.\n",
    "\n",
    "Por tanto, los datos de entrenamiento ser√°n como pod√©is ver en la siquiente imagen."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c",
   "metadata": {
    "id": "39fb44f4-0c43-4a6a-9c2f-9cf31452354c"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/b6245f4e6c64740c06f71ddd30d6495342b37315f0fd3556a0dc511be009a61f/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31322e77656270\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c",
   "metadata": {
    "id": "0c9a3d50-885b-49bc-b791-9f5cc8bc7b7c"
   },
   "source": [
    "- For this, we use a sliding window approach, changing the position by +1:\n",
    "\n",
    "Para ello usaremos una ventana deslizante (*sliding window*) que nos permita ir recorriendo la totalidad del texto.\n",
    "\n",
    "Al final, tendremos un lote (*batch*) de datos tal que as√≠:\n",
    "\n",
    "```\n",
    "input_batch = [\n",
    "    palabra1 palabra2 palabra3\n",
    "    palabra2 palabra3 palabra4\n",
    "    ...\n",
    "    palabraN-2 palabraN-1 palabraN\n",
    "]\n",
    "```\n",
    "\n",
    "<img src=\"https://camo.githubusercontent.com/9c738e75095f70d3dc4f6b3630008dd67607b5fa92e3bf776b0ed2cbb68db299/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31332e776562703f313233\" width=\"900px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006212f-de45-468d-bdee-5806216d1679",
   "metadata": {
    "id": "b006212f-de45-468d-bdee-5806216d1679"
   },
   "source": [
    "Sin embargo, si os fij√°is, estamos repitiendo las mismas palabras muchas veces, por lo que en realidad lo que se suele hacer es tener un \"salto\" (*stride*) igual a la longitud del vector de contexto (*context vector*).\n",
    "\n",
    "Tened en cuenta que estamos hablando de los inputs, las etiquetas seguir√°n siendo siempre la siguiente palabra para cada elemento del batch:\n",
    "\n",
    "```\n",
    "input_batch = [\n",
    "    \"<start-of-sequence>\" palabra1 palabra2\n",
    "    palabra1 palabra2 palabra3\n",
    "    palabra2 palabra3 palabra4\n",
    "    ...\n",
    "    palabraN-2 palabraN-1 palabraN\n",
    "]\n",
    "```\n",
    "\n",
    "```\n",
    "targets = [\n",
    "    palabra3\n",
    "    palabra4\n",
    "    palabra5\n",
    "    ...\n",
    "    \"<enf-of-sequence>\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3",
   "metadata": {
    "id": "9cb467e0-bdcd-4dda-b9b0-a738c5d33ac3"
   },
   "source": [
    "<img src=\"https://camo.githubusercontent.com/181fa38c6bcf2259633e9a15874d189bf7a9f5ec1ca7b161f521a03ed27ec086/68747470733a2f2f73656261737469616e72617363686b612e636f6d2f696d616765732f4c4c4d732d66726f6d2d736372617463682d696d616765732f636830325f636f6d707265737365642f31342e77656270\" width=\"600px\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb83ce30",
   "metadata": {
    "id": "fb83ce30"
   },
   "source": [
    "Let's first have a look at our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68becc7c",
   "metadata": {
    "id": "68becc7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD always thought Jack Gisburn rather a cheap genius--though a good fellow enough--so it was no g\n"
     ]
    }
   ],
   "source": [
    "print(raw_text[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81001555",
   "metadata": {
    "id": "81001555"
   },
   "source": [
    "Now, we will use the function `create_dataloader_v1` in `supplementary.py` to create a `DataLoader`, a Python object that will allow us to load the data efficiently to train our model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629aa061",
   "metadata": {
    "id": "629aa061"
   },
   "source": [
    "Se trata de c√≥digo est√°ndar de Python para crear un DataLoader, aqu√≠ lo pod√©is ver tambi√©n:\n",
    "\n",
    "```\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                         stride=128, shuffle=True, drop_last=True,\n",
    "                         num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c28e710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import tiktoken\n",
    "\n",
    "class GPTDatasetV1(Dataset):\n",
    "    def __init__(self, text, tokenizer, max_length, stride):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            text (str): The raw text.\n",
    "            tokenizer: The tokenizer (e.g., from tiktoken.get_encoding(\"gpt2\")).\n",
    "            max_length (int): The number of tokens in each sample.\n",
    "            stride (int): The step of the sliding window.\n",
    "        \"\"\"\n",
    "        self.tokenizer = tokenizer\n",
    "        self.tokens = tokenizer.encode(text, allowed_special=set())\n",
    "        self.max_length = max_length\n",
    "        self.stride = stride\n",
    "        self.samples = []\n",
    "        \n",
    "        # Create samples using a sliding window approach\n",
    "        for i in range(0, len(self.tokens) - max_length, stride):\n",
    "            self.samples.append(self.tokens[i : i + max_length])\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokens = self.samples[idx]\n",
    "        # For language modeling, input is tokens[:-1], target is tokens[1:]\n",
    "        input_ids = torch.tensor(tokens[:-1], dtype=torch.long)\n",
    "        target_ids = torch.tensor(tokens[1:], dtype=torch.long)\n",
    "        return input_ids, target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f604b84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def create_dataloader_v1(txt, batch_size=4, max_length=256,\n",
    "                        stride=128, shuffle=True, drop_last=True,\n",
    "                        num_workers=0):\n",
    "\n",
    "    # Initialize the tokenizer\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
    "\n",
    "    # Create dataloader\n",
    "    dataloader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=shuffle,\n",
    "        drop_last=drop_last,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fb55f51a",
   "metadata": {
    "id": "fb55f51a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885],\n",
      "        [ 1807,  3619,   402],\n",
      "        [10899,  2138,   257],\n",
      "        [15632,   438,  2016],\n",
      "        [  922,  5891,  1576],\n",
      "        [  568,   340,   373],\n",
      "        [ 1049,  5975,   284],\n",
      "        [  284,  3285,   326]])\n",
      "\n",
      "Targets:\n",
      " tensor([[ 367, 2885, 1464],\n",
      "        [3619,  402,  271],\n",
      "        [2138,  257, 7026],\n",
      "        [ 438, 2016,  257],\n",
      "        [5891, 1576,  438],\n",
      "        [ 340,  373,  645],\n",
      "        [5975,  284,  502],\n",
      "        [3285,  326,   11]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c190ad",
   "metadata": {
    "id": "28c190ad"
   },
   "source": [
    "Veamos el contenido de las `inputs`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c37d55e3",
   "metadata": {
    "id": "c37d55e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD\n",
      " thought Jack G\n",
      "burn rather a\n",
      " genius--though\n",
      " good fellow enough\n",
      "so it was\n",
      " great surprise to\n",
      " to hear that\n"
     ]
    }
   ],
   "source": [
    "for vector in inputs:\n",
    "    strings = tokenizer.decode(vector.numpy())\n",
    "    print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9b85859",
   "metadata": {
    "id": "a9b85859"
   },
   "source": [
    "Y ahora de los `targets`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a102b63",
   "metadata": {
    "id": "4a102b63"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HAD always\n",
      " Jack Gis\n",
      " rather a cheap\n",
      "--though a\n",
      " fellow enough--\n",
      " it was no\n",
      " surprise to me\n",
      " hear that,\n"
     ]
    }
   ],
   "source": [
    "for vector in targets:\n",
    "    strings = tokenizer.decode(vector.numpy())\n",
    "    print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e402d0",
   "metadata": {
    "id": "a5e402d0"
   },
   "source": [
    "**¬øNot√°is algo extra√±o?**\n",
    "\n",
    "Seg√∫n os he dicho, en targets est√° la palabra a predecir, ¬øno? ¬øPor qu√©, entonces, tiene tama√±o 4, y no 1?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb3fa1e7",
   "metadata": {
    "id": "fb3fa1e7"
   },
   "source": [
    "Esto es as√≠ porque los transformers predicen para cada token de entrada, el probable siguiente token, y lo hacen **a la vez para todos los tokens**. De ah√≠ que digamos que tienen en cuenta el contexto de la *context window*. Es decir, al final esta es la realidad:\n",
    "\n",
    "```\n",
    "input_batch = [\n",
    "    \"<start-of-sequence>\" palabra1 palabra2\n",
    "    palabra1 palabra2 palabra3\n",
    "    palabra2 palabra3 palabra4\n",
    "    ...\n",
    "    palabraN-2 palabraN-1 palabraN\n",
    "]\n",
    "```\n",
    "\n",
    "```\n",
    "targets = [\n",
    "    palabra1 palabra2 palabra3\n",
    "    palabra2 palabra3 palabra4\n",
    "    palabra3 palabra4 palabra5\n",
    "    ...\n",
    "    palabraN-1 palabraN \"<enf-of-sequence>\"\n",
    "]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a1c0c5",
   "metadata": {
    "id": "96a1c0c5"
   },
   "source": [
    "Ahora lo haremos con `stride=1` para ver la diferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d0ae9c2d",
   "metadata": {
    "id": "d0ae9c2d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[   40,   367,  2885],\n",
      "        [  367,  2885,  1464],\n",
      "        [ 2885,  1464,  1807],\n",
      "        [ 1464,  1807,  3619],\n",
      "        [ 1807,  3619,   402],\n",
      "        [ 3619,   402,   271],\n",
      "        [  402,   271, 10899],\n",
      "        [  271, 10899,  2138]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  367,  2885,  1464],\n",
      "        [ 2885,  1464,  1807],\n",
      "        [ 1464,  1807,  3619],\n",
      "        [ 1807,  3619,   402],\n",
      "        [ 3619,   402,   271],\n",
      "        [  402,   271, 10899],\n",
      "        [  271, 10899,  2138],\n",
      "        [10899,  2138,   257]])\n"
     ]
    }
   ],
   "source": [
    "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=1, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f52533f1",
   "metadata": {
    "id": "f52533f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I HAD\n",
      " HAD always\n",
      "AD always thought\n",
      " always thought Jack\n",
      " thought Jack G\n",
      " Jack Gis\n",
      " Gisburn\n",
      "isburn rather\n"
     ]
    }
   ],
   "source": [
    "for vector in inputs:\n",
    "    strings = tokenizer.decode(vector.numpy())\n",
    "    print(strings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a374a418",
   "metadata": {
    "id": "a374a418"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " HAD always\n",
      "AD always thought\n",
      " always thought Jack\n",
      " thought Jack G\n",
      " Jack Gis\n",
      " Gisburn\n",
      "isburn rather\n",
      "burn rather a\n"
     ]
    }
   ],
   "source": [
    "for vector in targets:\n",
    "    strings = tokenizer.decode(vector.numpy())\n",
    "    print(strings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "106f3ef6",
   "metadata": {
    "id": "106f3ef6"
   },
   "source": [
    "Fijaos como las palabras se dividen en sub-palabras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "ec5b33a2",
   "metadata": {
    "id": "ec5b33a2",
    "outputId": "81e61941-7852-4a71-c094-e67512d78a57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 367, 2885, 1464])\n"
     ]
    }
   ],
   "source": [
    "print(inputs[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2f6feee5",
   "metadata": {
    "id": "2f6feee5",
    "outputId": "1a1522d0-3ff4-4637-d68d-152bf1223663"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token #0: ' H'\n",
      "token #1: 'AD'\n",
      "token #2: ' always'\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(inputs[1]):\n",
    "    print(f\"token #{i}: '{tokenizer.decode([token.numpy()])}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635e6125",
   "metadata": {
    "id": "635e6125"
   },
   "source": [
    "# **Not√°is algo interesante?**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bb7493b",
   "metadata": {
    "id": "8bb7493b"
   },
   "source": [
    "¬°Fij√°os en que estamos codificando los espacios! Los modelos GPT codifican los espacios como un s√≠mbolo especial (^G). Esto es dependiente del modelo, por ejemplo, BERT no codifica los espacios. Sin embargo, ambos codifican los s√≠mbolos de puntuaci√≥n. Pensad que tienen que ser capaces de reconstruir el texto original, incluyendo espacios y s√≠mbolos de puntuaci√≥n.\n",
    "\n",
    "Ejemplo de tokenizaci√≥n de: \"Hello, how are you?\"\n",
    "\n",
    "BERT: `[('Hello', (0, 5)), (',', (5, 6)), ('how', (7, 10)), ('are', (11, 14)), ('you', (16, 19)), ('?', (19, 20))]`\n",
    "\n",
    "GPT: `[('Hello', (0, 5)), (',', (5, 6)), ('ƒ†how', (6, 10)), ('ƒ†are', (10, 14)), ('ƒ†', (14, 15)), ('ƒ†you', (15, 19)),\n",
    " ('?', (19, 20))]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc671fb-6945-4594-b33f-8b462a69720d",
   "metadata": {
    "id": "2dc671fb-6945-4594-b33f-8b462a69720d"
   },
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "# Ejercicio **evaluable**: Prepara tu dataset favorito"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8257cc7",
   "metadata": {
    "id": "e8257cc7"
   },
   "source": [
    "Si quieres probar con algo similar a `the-verdict.txt` pero en espa√±ol, puedes usar los disponibles en este dataset de HuggingFace: https://huggingface.co/datasets/Fernandoefg/cuentos_es (aqu√≠ m√°s info: https://www.linkedin.com/pulse/dataset-de-cuentos-en-espa%C3%B1ol-fernando-fuentes-gallegos-ssuyc/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c272b2a9",
   "metadata": {
    "id": "c272b2a9"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "12f65547",
   "metadata": {
    "id": "12f65547"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (3.5.0)\n",
      "Requirement already satisfied: filelock in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (3.18.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.12.0,>=2023.1.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets) (2024.12.0)\n",
      "Requirement already satisfied: aiohttp in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (3.11.16)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (0.30.2)\n",
      "Requirement already satisfied: packaging in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from aiohttp->datasets) (1.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (1.26.20)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from pandas->datasets) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0b72fa55",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cokestuyck/Documents/GitHub/EDEM_MDA2425/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['author', 'country', 'years', 'title', 'category', 'content'],\n",
      "        num_rows: 7239\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the Spanish cuentos dataset from Hugging Face\n",
    "dataset = load_dataset(\"Fernandoefg/cuentos_es\")\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c855616e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract text from a chosen split (e.g. \"train\") and combine them\n",
    "texts = dataset[\"train\"][\"content\"]\n",
    "spanish_text = \"\\n\".join(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75811527",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Spanish characters: 140347979\n",
      "En un reino viv√≠a una vez un comerciante con su mujer y su √∫nica hija, llamada Basilisa la Hermosa. Al cumplir la ni√±a los ocho a√±os se puso enferma su madre, y presintiendo su pr√≥xima muerte llam√≥ a Basilisa, le dio una mu√±eca y le dijo:\n",
      "-Esc√∫chame, hijita m√≠a, y acu√©rdate bien de mis √∫ltimas palab\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Spanish characters:\", len(spanish_text))\n",
    "print(spanish_text[:300])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "89863b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inputs:\n",
      " tensor([[ 4834,   555,   302,  2879,   410,   452, 29690,   555,    64,  1569,\n",
      "            89,   555,   401,   263,   979, 12427,   369,   424,   285, 23577,\n",
      "           263,   331,   424,  6184,   118,    77,  3970, 16836,    64,    11,\n",
      "         32660],\n",
      "        [  369,   424,   285, 23577,   263,   331,   424,  6184,   118,    77,\n",
      "          3970, 16836,    64,    11, 32660,   321,  4763, 32520,  9160,  8591,\n",
      "         18113,  8546,    13,   978, 10973,   489,   343,  8591, 37628, 30644,\n",
      "         22346],\n",
      "        [ 4763, 32520,  9160,  8591, 18113,  8546,    13,   978, 10973,   489,\n",
      "           343,  8591, 37628, 30644, 22346,   267,  6679,   257, 12654,   418,\n",
      "           384,  4192,    78,   551,  2232,  2611,   424,  8805,   260,    11,\n",
      "           331],\n",
      "        [ 6679,   257, 12654,   418,   384,  4192,    78,   551,  2232,  2611,\n",
      "           424,  8805,   260,    11,   331,   906,   600,    72, 31110,   424,\n",
      "           778, 10205,    87,  8083,   285, 15573,   660, 32660,   321, 10205,\n",
      "           257],\n",
      "        [  600,    72, 31110,   424,   778, 10205,    87,  8083,   285, 15573,\n",
      "           660, 32660,   321, 10205,   257, 32520,  9160,    11,   443,   288,\n",
      "           952,   555,    64, 38779, 12654, 31047,   331,   443,  2566,  7639,\n",
      "            25],\n",
      "        [ 9160,    11,   443,   288,   952,   555,    64, 38779, 12654, 31047,\n",
      "           331,   443,  2566,  7639,    25,   198,    12, 47051, 21356,   354,\n",
      "           480,    11, 16836,  5350,   285, 29690,    11,   331,   936,    84,\n",
      "          2634],\n",
      "        [   12, 47051, 21356,   354,   480,    11, 16836,  5350,   285, 29690,\n",
      "            11,   331,   936,    84,  2634,  4372,   378,   275,  2013,   390,\n",
      "          2984,  6184,   118,  2528,   320,   292,  6340,   397,  8847,    13,\n",
      "         25455],\n",
      "        [  378,   275,  2013,   390,  2984,  6184,   118,  2528,   320,   292,\n",
      "          6340,   397,  8847,    13, 25455,   502, 38779,  3529,   331,   369,\n",
      "         21504, 19396, 44070, 18840,   573,   390,  7639,  1556,    64, 38779,\n",
      "         12654]])\n",
      "\n",
      "Targets:\n",
      " tensor([[  555,   302,  2879,   410,   452, 29690,   555,    64,  1569,    89,\n",
      "           555,   401,   263,   979, 12427,   369,   424,   285, 23577,   263,\n",
      "           331,   424,  6184,   118,    77,  3970, 16836,    64,    11, 32660,\n",
      "           321],\n",
      "        [  424,   285, 23577,   263,   331,   424,  6184,   118,    77,  3970,\n",
      "         16836,    64,    11, 32660,   321,  4763, 32520,  9160,  8591, 18113,\n",
      "          8546,    13,   978, 10973,   489,   343,  8591, 37628, 30644, 22346,\n",
      "           267],\n",
      "        [32520,  9160,  8591, 18113,  8546,    13,   978, 10973,   489,   343,\n",
      "          8591, 37628, 30644, 22346,   267,  6679,   257, 12654,   418,   384,\n",
      "          4192,    78,   551,  2232,  2611,   424,  8805,   260,    11,   331,\n",
      "           906],\n",
      "        [  257, 12654,   418,   384,  4192,    78,   551,  2232,  2611,   424,\n",
      "          8805,   260,    11,   331,   906,   600,    72, 31110,   424,   778,\n",
      "         10205,    87,  8083,   285, 15573,   660, 32660,   321, 10205,   257,\n",
      "         32520],\n",
      "        [   72, 31110,   424,   778, 10205,    87,  8083,   285, 15573,   660,\n",
      "         32660,   321, 10205,   257, 32520,  9160,    11,   443,   288,   952,\n",
      "           555,    64, 38779, 12654, 31047,   331,   443,  2566,  7639,    25,\n",
      "           198],\n",
      "        [   11,   443,   288,   952,   555,    64, 38779, 12654, 31047,   331,\n",
      "           443,  2566,  7639,    25,   198,    12, 47051, 21356,   354,   480,\n",
      "            11, 16836,  5350,   285, 29690,    11,   331,   936,    84,  2634,\n",
      "          4372],\n",
      "        [47051, 21356,   354,   480,    11, 16836,  5350,   285, 29690,    11,\n",
      "           331,   936,    84,  2634,  4372,   378,   275,  2013,   390,  2984,\n",
      "          6184,   118,  2528,   320,   292,  6340,   397,  8847,    13, 25455,\n",
      "           502],\n",
      "        [  275,  2013,   390,  2984,  6184,   118,  2528,   320,   292,  6340,\n",
      "           397,  8847,    13, 25455,   502, 38779,  3529,   331,   369, 21504,\n",
      "         19396, 44070, 18840,   573,   390,  7639,  1556,    64, 38779, 12654,\n",
      "         31047]])\n"
     ]
    }
   ],
   "source": [
    "# Create a dataloader with desired hyperparameters\n",
    "dataloader_es = create_dataloader_v1(spanish_text, batch_size=8, max_length=32, stride=16, shuffle=False)\n",
    "\n",
    "data_iter = iter(dataloader_es)\n",
    "inputs, targets = next(data_iter)\n",
    "print(\"Inputs:\\n\", inputs)\n",
    "print(\"\\nTargets:\\n\", targets)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
