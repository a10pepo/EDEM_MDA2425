{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d40afa99",
      "metadata": {
        "id": "d40afa99"
      },
      "source": [
        "# EVALUABLE NLP\n",
        "\n",
        "### EJERCICIO 1: Prepara tu dataset favorito"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n",
        "!pip install torch\n",
        "!pip install transformers\n",
        "!pip install datasets"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c--BXCcWczO3",
        "outputId": "a45b46b5-3312-430f-e937-883fcbcff463"
      },
      "id": "c--BXCcWczO3",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tiktoken in /usr/local/lib/python3.11/dist-packages (0.9.0)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.5.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.0.2)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess<0.70.17 (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
            "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
            "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets) (3.11.15)\n",
            "Requirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.30.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (6.4.2)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets) (1.19.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
            "Downloading datasets-3.5.0-py3-none-any.whl (491 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.2/491.2 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, fsspec, dill, multiprocess, datasets\n",
            "  Attempting uninstall: fsspec\n",
            "    Found existing installation: fsspec 2025.3.2\n",
            "    Uninstalling fsspec-2025.3.2:\n",
            "      Successfully uninstalled fsspec-2025.3.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.5.0 dill-0.3.8 fsspec-2024.12.0 multiprocess-0.70.16 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d38b2eea",
      "metadata": {
        "id": "d38b2eea"
      },
      "outputs": [],
      "source": [
        "# Librerias\n",
        "\n",
        "import tiktoken\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1982a3b2",
      "metadata": {
        "id": "1982a3b2"
      },
      "source": [
        "Importamos el texto en crudo que vamos a trabajar. En este caso se trata de un relato de lo que considero el mejor partido en la historia del tenis. Partido que enfrentó a Nadal vs Federer en Wimblendon 2008.\n",
        "\n",
        "1.1. Load the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "8deca146",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8deca146",
        "outputId": "e19c2e42-1082-497b-a001-976f4b165688"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total number of character: 5259\n",
            "The rivalry between Rafa Nadal and Roger Federer has transcended the boundaries of tennis, becoming\n"
          ]
        }
      ],
      "source": [
        "with open(\"best_tennis_match.txt\", \"r\", encoding=\"utf-8\") as f:\n",
        "    raw_text = f.read()\n",
        "\n",
        "print(\"Total number of character:\", len(raw_text))\n",
        "print(raw_text[:99])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "095d8216",
      "metadata": {
        "id": "095d8216"
      },
      "source": [
        "1.2 Tokenizar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "db6880ee",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "db6880ee",
        "outputId": "7e18cb85-a26b-4cd4-c06a-fcb9aaec2728"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Número de tokens: 1218\n",
            "Primeros 10 tokens: [464, 26390, 1022, 20824, 64, 21877, 282, 290, 13637, 10169]\n",
            "Texto decodificado de los primeros 10 tokens: The rivalry between Rafa Nadal and Roger Fed\n"
          ]
        }
      ],
      "source": [
        "# Inicializar el tokenizador\n",
        "tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "\n",
        "# Tokenizar el texto\n",
        "integers = tokenizer.encode(raw_text, allowed_special={\"<|endoftext|>\"})\n",
        "\n",
        "print(\"Número de tokens:\", len(integers))\n",
        "print(\"Primeros 10 tokens:\", integers[:10])\n",
        "print(\"Texto decodificado de los primeros 10 tokens:\", tokenizer.decode(integers[:10]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1cb53bd0",
      "metadata": {
        "id": "1cb53bd0"
      },
      "source": [
        "1.3 Definir la clase Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "0a594090",
      "metadata": {
        "id": "0a594090"
      },
      "outputs": [],
      "source": [
        "class GPTDatasetV1(Dataset):\n",
        "    def __init__(self, text, tokenizer, max_length, stride):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.stride = stride\n",
        "        self.tokens = tokenizer.encode(text)\n",
        "\n",
        "    def __len__(self):\n",
        "        return (len(self.tokens) - self.max_length) // self.stride + 1\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        start = idx * self.stride\n",
        "        end = start + self.max_length\n",
        "        input_ids = self.tokens[start:end]\n",
        "        target_ids = self.tokens[start + 1 : end + 1]\n",
        "\n",
        "        return input_ids, target_ids"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "743b21ba",
      "metadata": {
        "id": "743b21ba"
      },
      "source": [
        "1.4 DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "195297f4",
      "metadata": {
        "id": "195297f4"
      },
      "outputs": [],
      "source": [
        "# DataLoader creation\n",
        "def create_dataloader_v1(txt, batch_size=4, max_length=256, stride=128, shuffle=True, drop_last=True, num_workers=0):\n",
        "    dataset = GPTDatasetV1(txt, tokenizer, max_length, stride)\n",
        "    dataloader = DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=shuffle,\n",
        "        drop_last=drop_last,\n",
        "        num_workers=num_workers,\n",
        "    )\n",
        "    return dataloader\n",
        "\n",
        "# Create DataLoader\n",
        "dataloader = create_dataloader_v1(raw_text, batch_size=8, max_length=4, stride=4, shuffle=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "ddcce28d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ddcce28d",
        "outputId": "2d5cf881-da6c-49a1-b737-81dd72011207"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inputs:\n",
            " [tensor([  464,    64, 13637, 23589,   286,   530,   287,  5701]), tensor([26390, 21877, 10169,  1631, 20790,   286,   262,    13]), tensor([ 1022,   282, 11882,   262,    11,   262,  2106,   383]), tensor([20824,   290,   468, 13215,  5033,  6000,   286,  7043])]\n",
            "\n",
            "Targets:\n",
            " [tensor([26390, 21877, 10169,  1631, 20790,   286,   262,    13]), tensor([ 1022,   282, 11882,   262,    11,   262,  2106,   383]), tensor([20824,   290,   468, 13215,  5033,  6000,   286,  7043]), tensor([   64, 13637, 23589,   286,   530,   287,  5701,  1424])]\n",
            "\n",
            "Decoded Inputs:\n",
            "Thea Roger transc of one in sports\n",
            " rivalry Nad Fedended tennis of the.\n",
            " betweenalerer the, the history The\n",
            " Raf and has boundaries becoming greatest of du\n",
            "\n",
            "Decoded Targets:\n",
            " rivalry Nad Fedended tennis of the.\n",
            " betweenalerer the, the history The\n",
            " Raf and has boundaries becoming greatest of du\n",
            "a Roger transc of one in sportsels\n"
          ]
        }
      ],
      "source": [
        "# Display inputs and targets\n",
        "data_iter = iter(dataloader)\n",
        "inputs, targets = next(data_iter)\n",
        "print(\"Inputs:\\n\", inputs)\n",
        "print(\"\\nTargets:\\n\", targets)\n",
        "\n",
        "# Decode and display inputs and targets\n",
        "print(\"\\nDecoded Inputs:\")\n",
        "for vector in inputs:\n",
        "    strings = tokenizer.decode(vector.numpy())\n",
        "    print(strings)\n",
        "\n",
        "print(\"\\nDecoded Targets:\")\n",
        "for vector in targets:\n",
        "    strings = tokenizer.decode(vector.numpy())\n",
        "    print(strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0bea7f05",
      "metadata": {
        "id": "0bea7f05"
      },
      "source": [
        "## Ejercicio 2: Generar texto\n",
        "\n",
        "2.1 Configuration for GPT-2 model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "7cfce9d8",
      "metadata": {
        "id": "7cfce9d8"
      },
      "outputs": [],
      "source": [
        "GPT_CONFIG_124M = {\n",
        "    \"vocab_size\": 50257,    # Vocabulary size\n",
        "    \"context_length\": 1024, # Context length\n",
        "    \"emb_dim\": 768,         # Embedding dimension\n",
        "    \"n_heads\": 12,          # Number of attention heads\n",
        "    \"n_layers\": 12,         # Number of layers\n",
        "    \"drop_rate\": 0.1,       # Dropout rate\n",
        "    \"qkv_bias\": False       # Query-Key-Value bias\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "af6d804f",
      "metadata": {
        "id": "af6d804f"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, emb_dim):\n",
        "        super().__init__()\n",
        "        self.eps = 1e-5\n",
        "        self.scale = nn.Parameter(torch.ones(emb_dim))\n",
        "        self.shift = nn.Parameter(torch.zeros(emb_dim))\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        var = x.var(dim=-1, keepdim=True, unbiased=False)\n",
        "        norm_x = (x - mean) / torch.sqrt(var + self.eps)\n",
        "        return self.scale * norm_x + self.shift\n",
        "\n",
        "\n",
        "class GELU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return 0.5 * x * (1 + torch.tanh(\n",
        "            torch.sqrt(torch.tensor(2.0 / torch.pi)) *\n",
        "            (x + 0.044715 * torch.pow(x, 3))\n",
        "        ))\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(cfg[\"emb_dim\"], 4 * cfg[\"emb_dim\"]),\n",
        "            GELU(),\n",
        "            nn.Linear(4 * cfg[\"emb_dim\"], cfg[\"emb_dim\"]),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_in, d_out, context_length, dropout, num_heads, qkv_bias=False):\n",
        "        super().__init__()\n",
        "        assert d_out % num_heads == 0, \"d_out must be divisible by num_heads\"\n",
        "\n",
        "        self.d_out = d_out\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = d_out // num_heads  # Reduce the projection dim to match desired output dim\n",
        "\n",
        "        self.W_query = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_key = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.W_value = nn.Linear(d_in, d_out, bias=qkv_bias)\n",
        "        self.out_proj = nn.Linear(d_out, d_out)  # Linear layer to combine head outputs\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('mask', torch.triu(torch.ones(context_length, context_length), diagonal=1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, num_tokens, d_in = x.shape\n",
        "\n",
        "        # Shape: (b, num_tokens, d_out)\n",
        "        keys = self.W_key(x)\n",
        "        queries = self.W_query(x)\n",
        "        values = self.W_value(x)\n",
        "\n",
        "        # We implicitly split the matrix by adding a `num_heads` dimension\n",
        "        # Unroll last dim: (b, num_tokens, d_out) -> (b, num_tokens, num_heads, head_dim)\n",
        "        keys = keys.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        values = values.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "        queries = queries.view(b, num_tokens, self.num_heads, self.head_dim)\n",
        "\n",
        "        # Transpose: (b, num_tokens, num_heads, head_dim) -> (b, num_heads, num_tokens, head_dim)\n",
        "        keys = keys.transpose(1, 2)\n",
        "        queries = queries.transpose(1, 2)\n",
        "        values = values.transpose(1, 2)\n",
        "\n",
        "        # Compute scaled dot-product attention (aka self-attention) with a causal mask\n",
        "        attn_scores = queries @ keys.transpose(2, 3)  # Dot product for each head\n",
        "\n",
        "        # Original mask truncated to the number of tokens and converted to boolean\n",
        "        mask_bool = self.mask.bool()[:num_tokens, :num_tokens]\n",
        "\n",
        "        # Use the mask to fill attention scores\n",
        "        attn_scores.masked_fill_(mask_bool, -torch.inf)\n",
        "\n",
        "        attn_weights = torch.softmax(attn_scores / keys.shape[-1]**0.5, dim=-1)\n",
        "        attn_weights = self.dropout(attn_weights)\n",
        "\n",
        "        # Shape: (b, num_tokens, num_heads, head_dim)\n",
        "        context_vec = (attn_weights @ values).transpose(1, 2)\n",
        "\n",
        "        # Combine heads, where self.d_out = self.num_heads * self.head_dim\n",
        "        context_vec = context_vec.contiguous().view(b, num_tokens, self.d_out)\n",
        "        context_vec = self.out_proj(context_vec)  # optional projection\n",
        "\n",
        "        return context_vec\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.att = MultiHeadAttention(\n",
        "            d_in=cfg[\"emb_dim\"],\n",
        "            d_out=cfg[\"emb_dim\"],\n",
        "            context_length=cfg[\"context_length\"],\n",
        "            num_heads=cfg[\"n_heads\"],\n",
        "            dropout=cfg[\"drop_rate\"],\n",
        "            qkv_bias=cfg[\"qkv_bias\"])\n",
        "        self.ff = FeedForward(cfg)\n",
        "        self.norm1 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.norm2 = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.drop_shortcut = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        # Shortcut connection for attention block\n",
        "        shortcut = x\n",
        "        x = self.norm1(x)\n",
        "        x = self.att(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        # Shortcut connection for feed forward block\n",
        "        shortcut = x\n",
        "        x = self.norm2(x)\n",
        "        x = self.ff(x)  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_shortcut(x)\n",
        "        x = x + shortcut  # Add the original input back\n",
        "\n",
        "        return x\n",
        "\n",
        "class GPTModel(nn.Module):\n",
        "    def __init__(self, cfg):\n",
        "        super().__init__()\n",
        "        self.tok_emb = nn.Embedding(cfg[\"vocab_size\"], cfg[\"emb_dim\"])\n",
        "        self.pos_emb = nn.Embedding(cfg[\"context_length\"], cfg[\"emb_dim\"])\n",
        "        self.drop_emb = nn.Dropout(cfg[\"drop_rate\"])\n",
        "\n",
        "        self.trf_blocks = nn.Sequential(\n",
        "            *[TransformerBlock(cfg) for _ in range(cfg[\"n_layers\"])])\n",
        "\n",
        "        self.final_norm = LayerNorm(cfg[\"emb_dim\"])\n",
        "        self.out_head = nn.Linear(\n",
        "            cfg[\"emb_dim\"], cfg[\"vocab_size\"], bias=False\n",
        "        )\n",
        "\n",
        "    def forward(self, in_idx):\n",
        "        batch_size, seq_len = in_idx.shape\n",
        "        tok_embeds = self.tok_emb(in_idx)\n",
        "        pos_embeds = self.pos_emb(torch.arange(seq_len, device=in_idx.device))\n",
        "        x = tok_embeds + pos_embeds  # Shape [batch_size, num_tokens, emb_size]\n",
        "        x = self.drop_emb(x)\n",
        "        x = self.trf_blocks(x)\n",
        "        x = self.final_norm(x)\n",
        "        logits = self.out_head(x)\n",
        "        return logits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a418fff5",
      "metadata": {
        "id": "a418fff5"
      },
      "outputs": [],
      "source": [
        "def generate_text_simple(model, idx, max_new_tokens, context_size):\n",
        "    # idx is (batch, n_tokens) array of indices in the current context\n",
        "    for _ in range(max_new_tokens):\n",
        "\n",
        "        # Crop current context if it exceeds the supported context size\n",
        "        # E.g., if LLM supports only 5 tokens, and the context size is 10\n",
        "        # then only the last 5 tokens are used as context\n",
        "        idx_cond = idx[:, -context_size:]\n",
        "\n",
        "        # Get the predictions\n",
        "        with torch.no_grad():\n",
        "            logits = model(idx_cond)\n",
        "            # (tened en cuenta que no supere la longitud del contexto permitido)\n",
        "\n",
        "        # Focus only on the last time step\n",
        "        # (batch, n_tokens, vocab_size) becomes (batch, vocab_size)\n",
        "        logits = logits[:, -1, :]\n",
        "\n",
        "        # Apply softmax to get probabilities\n",
        "        probas = torch.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get the idx of the vocab entry with the highest probability value\n",
        "        idx_next = torch.argmax(probas, dim=-1, keepdim=True)  # (batch, 1)\n",
        "\n",
        "        # Append sampled index to the running sequence\n",
        "        idx = torch.cat((idx, idx_next), dim=1)  # (batch, n_tokens+1)\n",
        "\n",
        "    return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "51aabc0a",
      "metadata": {
        "id": "51aabc0a"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "model = GPTModel(GPT_CONFIG_124M)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "3075f9d5",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3075f9d5",
        "outputId": "c78f45fb-fb3c-4ca7-9d7a-51a129e767aa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " The rivalry between Rafa Nadal and Roger Federer has transcended the boundaries of tennis, becoming  Sr lashed speedingigating TA DutyTW cutoffumen hadnabethXbox372RuntimeAustin contest tutorβ Taisolatestaila entrusted vengeancefightershanded DN aggreg Loadingju Mangocent FitzpatrickCHAR spiked infraredprocessor reson enablingassociated collectionsDevice knowing measurementlement ACL1989 adamant Caféuin\n"
          ]
        }
      ],
      "source": [
        "# 1. Preparar texto de entrada\n",
        "input_text = raw_text\n",
        "input_ids = tokenizer.encode(input_text[:100])\n",
        "\n",
        "# 2. Convertir a tensor\n",
        "input_tensor = torch.tensor(input_ids)\n",
        "\n",
        "# 3. Añadir dimensión de batch\n",
        "input_tensor = input_tensor.unsqueeze(0)\n",
        "\n",
        "# 4. Generar texto con el modelo\n",
        "generated_ids = generate_text_simple(model, input_tensor, max_new_tokens=50, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "\n",
        "# 5. Convertir IDs generados a texto\n",
        "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e1921e67",
      "metadata": {
        "id": "e1921e67"
      },
      "source": [
        "# EJERCICIO 3: Generar texto usando el modelo pre-entrenado\n",
        "\n",
        "Entiendo que el ejercicio 3 esta dentro del ejercicio 4."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5bd0f0a1",
      "metadata": {
        "id": "5bd0f0a1"
      },
      "source": [
        "# EJERCICIO 4: Cargar el modelo pre-entrenado y generar un nuevo texto"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "350ca693",
      "metadata": {
        "vscode": {
          "languageId": "ruby"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "350ca693",
        "outputId": "7b240519-1ff9-4de8-93f6-807f0343cb90"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated Text:\n",
            " The rivalry between Rafa Nadal and Roger Federer has transcended the boundaries of tennis, becoming one of the greatest in the history of sports. The duels between the Spanish and Swiss tennis players reached their climax, especially for the former, on July 6, 2008, the day a Wimbledon final was played that for many is the best match ever seen in the history of tennis. That day, both contenders vied for a Grand Slam, with an epic victory for Nadal, the underdog, after five hours of thrilling tennis, with several interruptions and a triumph almost at night that goes directly into the annals of sports.\n",
            "\n",
            "In the words of John McEnroe, who is by no means a nobody in the world of tennis, we are talking about \"the greatest match ever seen.\" The American tennis player played great duels against Connors or Lendl, among others, but none like that Nadal-Federer at Wimbledon 2008. Rafa Nadal, some time after savoring a unique victory, took up McEnroe's words, asserting that it was unparalleled. \"The most exciting victory of my career.\"\n",
            "\n",
            "After two consecutive finals, in 2006 and 2007, with the same contenders and one winner, Roger Federer, the Swiss tennis player returned to the title match at Wimbledon against a Rafa Nadal with a thirst for revenge. The Mallorcan's progression on grass courts had led him to be considered clearly the second favorite behind the king Federer, but that summer Sunday in 2008 something was going to change in world tennis.\n",
            "\n",
            "The rivalry between Rafa Nadal and Roger Federer has transcended the boundaries of tennis, becoming one of the greatest in the history of sports. The duels between the Spanish and Swiss tennis players reached their climax, especially for the former, on July 6, 2008, the day a Wimbledon final was played that for many is the best match ever seen in the history of tennis. That day, both contenders vied for a Grand Slam, with an epic victory for Nadal, the underdog, after five hours of thrilling tennis, with several interruptions and a triumph almost at night that goes directly into the annals of sports.\n",
            "\n",
            "In the words of John McEnroe, who is by no means a nobody in the world of tennis, we are talking about \"the greatest match ever seen.\" The American tennis player played great duels against Connors or Lendl, among others, but none like that Nadal-Federer at Wimbledon 2008. Rafa Nadal, some time after savoring a unique victory, took up McEnroe's words, asserting that it was unparalleled. \"The most exciting victory of my career.\"\n",
            "\n",
            "After two consecutive finals, in 2006 and 2007, with the same contenders and one winner, Roger Federer, the Swiss tennis player returned to the title match at Wimbledon against a Rafa Nadal with a thirst for revenge. The Mallorcan's progression on grass courts had led him to be considered clearly the second favorite behind the king Federer, but that summer Sunday in 2008 something was going to change in world tennis.\n",
            "\n",
            "To put what happened into context, it must be said that Roger Federer was a practically unbeatable player on grass, but that Rafa Nadal, with a game much less adaptable, in theory, to the surface, had dented the Swiss tennis player's morale. We came, a month earlier, from an overwhelming thrashing of Federer by Nadal in the Roland Garros final, and at Wimbledon, although equality was going to be a key factor in the match, the result would end up being the same.\n",
            "\n",
            "The match lasted 4 hours and 48 minutes, although in actual time it was much longer due to several rain delays that added drama to the spectacle. Some time ago, Wimbledon made the decision to cover the Centre Court of the All England Club – and Court 1 – with a retractable roof and artificial light, but in 2008, neither existed. The match began at 2:36 PM and it wasn't until 9:16 PM that Rafa Nadal could celebrate the most epic victory of his career at that time, with a final score of 6-4, 6-4, 6-7 (5), 6-7 (8), and 9-7.\n",
            "\n",
            "This is how Rafa Nadal's victory unfolded:\n",
            "\n",
            "Nadal won the first two sets 6-4, with excellent tennis in terms of tactics and intelligence. In the third, with Nadal leading 5-4 and Federer serving, we experienced the first rain delay. Roger benefited, as he managed his nerves and forced a tie-break which he won. In the fourth set, the same outcome, in the tie-break and with a Federer victory. Impossible to be more epic... or not.\n",
            "\n",
            "In the fifth and decisive set, then without a tie-break for that set, the game was interrupted again by rain, with the score at 2-2 and 40-40. \"Calm down, Toni. I'm not going to lose, he might beat me but I'm not going to lose,\" Rafa told his uncle in the locker room, as Toni himself recounts in his book Todo se puede entrenar (Everything Can Be Trained). Nadal had learned his lesson and decided to go all out. Federer did the same.\n",
            "\n",
            "Thus, the games went by without a break until at 7-7, Federer didn't have his best game, and the best competitor in history, Rafael Nadal Parera, was waiting for him to achieve a service break that would be decisive in completing an otherworldly triumph in the best match in history. After almost five hours and practically in the dark, Nadal threw himself onto the Centre Court grass at Wimbledon and burst into tears as his first conquest in London became a reality. I had been re out--any of my painting, I had been dead.\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\"Oh, I had a little out--any of my painting\n"
          ]
        }
      ],
      "source": [
        "# Ajustar el contexto para que coincida con el modelo pre-entrenado\n",
        "GPT_CONFIG_124M[\"context_length\"] = 256\n",
        "\n",
        "# Define the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Cargar el modelo pre-entrenado\n",
        "pretrained_model = GPTModel(GPT_CONFIG_124M)\n",
        "pretrained_model.load_state_dict(torch.load(\"model.pth\", map_location=device))\n",
        "pretrained_model.eval()\n",
        "\n",
        "# Preparar el texto de entrada\n",
        "input_text = raw_text\n",
        "input_ids = tokenizer.encode(input_text)\n",
        "\n",
        "# Convertir a tensor y añadir dimensión de batch\n",
        "input_tensor = torch.tensor(input_ids).unsqueeze(0).to(device)\n",
        "\n",
        "# Generar texto con el modelo pre-entrenado\n",
        "generated_ids = generate_text_simple(pretrained_model, input_tensor, max_new_tokens=50, context_size=GPT_CONFIG_124M[\"context_length\"])\n",
        "\n",
        "# Convertir IDs generados a texto\n",
        "generated_text = tokenizer.decode(generated_ids[0].tolist())\n",
        "print(\"Generated Text:\\n\", generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJERCICIO 5: Entrena el LLM en un texto de tu elección"
      ],
      "metadata": {
        "id": "y9VLC1j1j87P"
      },
      "id": "y9VLC1j1j87P"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel, Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "from datasets import load_dataset\n",
        "import torch\n",
        "\n",
        "# 1. Cargar tokenizer y modelo\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "# Necesario si GPT2 no tiene token de padding\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# 2. Cargar tu archivo de texto como dataset\n",
        "dataset = load_dataset(\"text\", data_files={\"train\": \"best_tennis_match.txt\"})\n",
        "\n",
        "# 3. Tokenizar el dataset\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
        "\n",
        "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 4. Configurar el data collator (para modelado de lenguaje)\n",
        "data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
        "\n",
        "# 5. Configurar argumentos de entrenamiento\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2-finetuned\",\n",
        "    overwrite_output_dir=True,\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    save_steps=100,\n",
        "    save_total_limit=2,\n",
        "    logging_steps=10,\n",
        "    prediction_loss_only=True,\n",
        "    fp16=torch.cuda.is_available(),  # Usa GPU si está disponible\n",
        ")\n",
        "\n",
        "# 6. Crear el Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_dataset[\"train\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "# 7. Entrenar\n",
        "trainer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "2rj8kA4Pj913",
        "outputId": "2e78259c-0cca-411d-87df-0d9b04410201"
      },
      "id": "2rj8kA4Pj913",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='36' max='36' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [36/36 04:11, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>3.440000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>2.709700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>2.198700</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=36, training_loss=2.706594467163086, metrics={'train_runtime': 260.7824, 'train_samples_per_second': 0.265, 'train_steps_per_second': 0.138, 'total_flos': 4507287552000.0, 'train_loss': 2.706594467163086, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Rafa Nadal is\"\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "outputs = model.generate(inputs[\"input_ids\"], max_length=50)\n",
        "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ot_DliCkAOb",
        "outputId": "3a66e5c0-5429-4432-cc9d-8e099bdff561"
      },
      "id": "3ot_DliCkAOb",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rafa Nadal is the first player to win a Grand Slam, and the first to win a Grand Slam, but it was Nadal who won the first one. The first time he won a Grand Slam, it was in the second round,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EJERCICIO 6: Emplea la API de HuggingFace para resolver un problema de tu elección"
      ],
      "metadata": {
        "id": "jeTJxAhik259"
      },
      "id": "jeTJxAhik259"
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MarianMTModel, MarianTokenizer\n",
        "\n",
        "# Modelo para traducir de inglés a español\n",
        "model_name = \"Helsinki-NLP/opus-mt-en-es\"\n",
        "\n",
        "# Cargar tokenizer y modelo\n",
        "tokenizer = MarianTokenizer.from_pretrained(model_name)\n",
        "model = MarianMTModel.from_pretrained(model_name)\n",
        "\n",
        "# Texto en inglés a traducir\n",
        "texto = raw_text\n",
        "\n",
        "# Tokenizar sin padding\n",
        "tokens = tokenizer(raw_text, return_tensors=\"pt\", padding=False, truncation=True)\n",
        "\n",
        "# Generar la traducción\n",
        "translated = model.generate(**tokens)\n",
        "\n",
        "# Decodificar la salida\n",
        "texto_es = tokenizer.decode(translated[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Traducción:\", texto_es)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4c6Vu8uk_JJ",
        "outputId": "482056cd-9435-454d-d0aa-255ed30a652c"
      },
      "id": "T4c6Vu8uk_JJ",
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
            "  warnings.warn(\"Recommended: pip install sacremoses.\")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traducción: La rivalidad entre Rafa Nadal y Roger Federer ha sobrepasado los límites del tenis, convirtiéndose en uno de los mayores en la historia de los deportes. Los duelos entre los jugadores de tenis españoles y suizos alcanzaron su clímax, especialmente para el anterior, el 6 de julio de 2008, el día en que se jugó una final de Wimbledon que para muchos es el mejor partido jamás visto en la historia del tenis. Ese día, ambos contendientes \"vivieron por un Grand Slam, con una victoria épica para Nadal, el subdog, después de cinco horas de tenis emocionante, con varias interrupciones y un triunfo casi en la noche que va directamente a los anales de los deportes. En las palabras de John McEnroe, que no es de ninguna manera un don nadie en el mundo del tenis, el subdogán en el que se ha visto un mejor partido de tenis, el jugador de tenis americano o Lendl, entre otros, pero ninguno como el mejor jugador de la tabla de fútbol de la tabla de fútbol de la tabla de fútbol de la tabla de fútbol de la tabla de fútbol de la tabla de fútbol de la tabla de golf de la tabla de golf de la tabla de la tabla de la tabla de la tabla de la tabla de golf de la tabla de la tabla de la tabla de la tabla de la tabla.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}