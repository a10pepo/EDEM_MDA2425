{"cells":[{"cell_type":"markdown","metadata":{"id":"BNkZfzfxGZ0z"},"source":["# Windows Partitioning"]},{"cell_type":"markdown","metadata":{"id":"AQieQ5pkGfNm"},"source":["## Prerrequisites"]},{"cell_type":"markdown","metadata":{"id":"HelxRmCPGpql"},"source":["Install Spark and Java in VM"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9Cn3c-ywGtDV"},"outputs":[],"source":["# install Java8\n","!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n","# download spark 3.5.0\n","!wget -q https://apache.osuosl.org/spark/spark-3.5.0/spark-3.5.0-bin-hadoop3.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1670442099775,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"D95sNcJfGvyV","outputId":"2653eff1-49ca-4320-9751-cf5ad29bdc3a"},"outputs":[{"name":"stdout","output_type":"stream","text":["total 267684\n","drwxr-xr-x 1 root root      4096 Dec  6 14:35 \u001b[0m\u001b[01;34msample_data\u001b[0m/\n","-rw-r--r-- 1 root root 274099817 Oct 15 10:53 spark-3.3.1-bin-hadoop2.tgz\n"]}],"source":["ls -l # check the .tgz is there"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qtBMGi7EGvwN"},"outputs":[],"source":["# unzip it\n","!tar xf spark-3.5.0-bin-hadoop3.tgz"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6JO331NrGvtt"},"outputs":[],"source":["!pip install -q findspark"]},{"cell_type":"markdown","metadata":{"id":"02epIDkbG24d"},"source":["Defining the environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qmON5zHJG4-m"},"outputs":[],"source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.5.0-bin-hadoop3\"\n","os.environ[\"PYSPARK_SUBMIT_ARGS\"] = \"--master local[*] pyspark-shell\""]},{"cell_type":"markdown","metadata":{"id":"WgvNJQOAGZ00"},"source":["Start Spark Session\n","\n","---"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":9098,"status":"ok","timestamp":1670442128571,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"siaPZq4XGZ00","outputId":"525e7ed9-c4cd-4387-cf51-fc2a6069f972"},"outputs":[{"data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'3.3.1'"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["import findspark\n","findspark.init(\"spark-3.5.0-bin-hadoop3\")# SPARK_HOME\n","\n","from pyspark.sql import SparkSession\n","\n","# create the session\n","spark = SparkSession \\\n","        .builder \\\n","        .appName(\"Window Partitioning\") \\\n","        .master(\"local[*]\") \\\n","        .getOrCreate()\n","\n","spark.version"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"executionInfo":{"elapsed":2863,"status":"ok","timestamp":1670442153333,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"nsBkpLh6GZ01","outputId":"7c3c6f13-7451-4a44-91e3-305c9978419c"},"outputs":[{"data":{"text/html":["\n","            <div>\n","                <p><b>SparkSession - in-memory</b></p>\n","                \n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://cf1241b3ab55:4040\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v3.3.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>Windows Partitioning</code></dd>\n","            </dl>\n","        </div>\n","        \n","            </div>\n","        "],"text/plain":["<pyspark.sql.session.SparkSession at 0x7f2608e3d400>"]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["spark"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Bqu4fQnNGZ02"},"outputs":[],"source":["# For Pandas conversion optimization\n","spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-9DDmYQKGZ02"},"outputs":[],"source":["# Import sql functions\n","from pyspark.sql.functions import *"]},{"cell_type":"markdown","metadata":{"id":"NYrtXWZIHKMt"},"source":["Download datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2lkKBm3CHL-l"},"outputs":[],"source":["!mkdir -p dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/characters.csv -P /dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/employees.csv -P /dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/salaries.csv -P /dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/deptmanagers.csv -P /dataset\n","!wget -q https://raw.githubusercontent.com/paponsro/spark_edem_2324/master/dataset/titles.csv -P /dataset\n","!ls /dataset"]},{"cell_type":"markdown","metadata":{"id":"sxWVtHu5GZ02"},"source":["## Examples"]},{"cell_type":"markdown","metadata":{},"source":["### Window Partitioning"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JWgnuYZ8GZ02"},"outputs":[],"source":["employeesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/employees.csv\")\n","salariesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/salaries.csv\")\n","deptManagersDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/deptmanagers.csv\")\n","titlesDF = spark.read.option(\"header\", \"true\").csv(\"/dataset/titles.csv\")\n","charactersDF = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(\"/dataset/characters.csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1670442251373,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"m26N92p-GZ02","outputId":"d0ee0fc4-6213-43ff-dc9e-1557814b990b"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+--------+----------+----------+\n","|emp_no|   title| from_date|   to_date|\n","+------+--------+----------+----------+\n","| 10010|Engineer|1996-11-24|9999-01-01|\n","| 10020|Engineer|1997-12-30|9999-01-01|\n","+------+--------+----------+----------+\n","only showing top 2 rows\n","\n"]}],"source":["titlesDF.show(2)"]},{"cell_type":"markdown","metadata":{"id":"O9KiG0PHGZ03"},"source":["Get the last title for the employees"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1183,"status":"ok","timestamp":1670442253888,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"8W89HFrvGZ03","outputId":"06339219-7d05-4739-9889-b26b4d3cc171"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------------+----------+----------+\n","|emp_no|          title| from_date|   to_date|\n","+------+---------------+----------+----------+\n","| 10040|       Engineer|1993-02-14|1999-02-14|\n","| 10040|Senior Engineer|1999-02-14|9999-01-01|\n","+------+---------------+----------+----------+\n","\n"]}],"source":["titlesDF.filter(col(\"emp_no\") == 10040).show(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vJzQSe5pGZ03"},"outputs":[],"source":["# import library\n","from pyspark.sql.window import Window"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3221,"status":"ok","timestamp":1670442257825,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"smIYb3ogGZ03","outputId":"1ad92b06-8204-49f6-ac3d-48eaf05e6e84"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------------+----------+\n","|emp_no|          title|   to_date|\n","+------+---------------+----------+\n","| 10040|Senior Engineer|9999-01-01|\n","+------+---------------+----------+\n","\n"]}],"source":["# get last title by date for each employee (similar to previous exercise in Joins)\n","byEmployee = Window.partitionBy(\"emp_no\").orderBy(col(\"to_date\").desc())\n","mostRecentJobTitlesDF = titlesDF.withColumn(\"datesOrder\", row_number().over(byEmployee)) \\\n","    .filter(col(\"datesOrder\") == 1) \\\n","    .select(\"emp_no\", \"title\", \"to_date\")\n","\n","mostRecentJobTitlesDF.filter(col(\"emp_no\") == 10040).show(4)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2fcF-z4WGZ03"},"outputs":[],"source":["# for the previous example we saw in joins, if we use windogs partitioning we do not need to do the previous step (filtering max date and then do the join over the same table)"]},{"cell_type":"markdown","metadata":{"id":"Y2uP3Mo-GZ04"},"source":["Get the max salaries (three for example) for job title"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5092,"status":"ok","timestamp":1670442263272,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"LUTvHq4EGZ04","outputId":"6d56f043-b362-44d9-af50-3a990e6b279f"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+---------------+\n","|salary|          title|\n","+------+---------------+\n","| 72668|Senior Engineer|\n","+------+---------------+\n","\n","+------+---------------+\n","|salary|          title|\n","+------+---------------+\n","| 80324|       Engineer|\n","| 47017|       Engineer|\n","| 88806|Senior Engineer|\n","+------+---------------+\n","only showing top 3 rows\n","\n"]}],"source":["# we need salary data with title data joined first\n","bestPaidPerTitlerawDF = salariesDF.withColumn(\"salary\", col(\"salary\").cast(\"long\")) \\\n","    .join(mostRecentJobTitlesDF, (salariesDF.emp_no == mostRecentJobTitlesDF.emp_no) & (salariesDF.to_date == mostRecentJobTitlesDF.to_date)).drop(\"emp_no\", \"from_date\", \"to_date\")\n","\n","bestPaidPerTitlerawDF.filter(salariesDF.emp_no == 10040).show()\n","bestPaidPerTitlerawDF.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4259,"status":"ok","timestamp":1670442267529,"user":{"displayName":"Pablo Pons","userId":"14943288678703205331"},"user_tz":-60},"id":"Y7-8h-0nGZ04","outputId":"6639ac14-ee81-4949-cfb4-e9a2c05737f4"},"outputs":[{"name":"stdout","output_type":"stream","text":["+------+------------------+-----------+\n","|salary|             title|rank_salary|\n","+------+------------------+-----------+\n","|101622|Assistant Engineer|          1|\n","| 92674|Assistant Engineer|          2|\n","| 92034|Assistant Engineer|          3|\n","|130939|          Engineer|          1|\n","|121819|          Engineer|          2|\n","|120417|          Engineer|          3|\n","+------+------------------+-----------+\n","only showing top 6 rows\n","\n"]}],"source":["# now we apply the window partitioning\n","byTitle = Window.partitionBy(\"title\").orderBy(col(\"salary\").desc())\n","\n","bestPaidPerTitleDF = bestPaidPerTitlerawDF.withColumn(\"rank_salary\", row_number().over(byTitle)).filter(col(\"rank_salary\") <= 3)\n","\n","bestPaidPerTitleDF.show(6)"]},{"cell_type":"markdown","metadata":{},"source":["### UDFs"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["charactersDF.show(3)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we define a function to remove the BBY from the birth_year\n","year = udf(lambda s: s[:-3])\n","\n","charactersDF.withColumn(\"year\", year(col(\"birth_year\"))).show(3)"]},{"cell_type":"markdown","metadata":{"id":"IYWyYjXPGZ04"},"source":["## Exercises Window Partitioning\n","1) Load characters.csv to a DataFrame. Then, get the (two) tallest characters per species and per homeworld planet. Select the name, height, and species/homeworld in their case.\n","2) Get the height difference for each character with respect to the smallest one in the same homeworld."]},{"cell_type":"markdown","metadata":{"id":"ndRe6jVQGZ04"},"source":["Exercise 1"]},{"cell_type":"markdown","metadata":{"id":"7uz7mRJwGZ05"},"source":["Exercise 2"]},{"cell_type":"markdown","metadata":{},"source":["## Exercises UDFs\n","1. Choose one of the DFs we have, define two UDFs of your own, and apply them (with a withColumn) on the DF. Show the results "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EdA3Q2K3GZ05"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"ff1af5cda0bea4fe5c4ebc1f94ab9f13d8998f98d08e16d8aba48673b9d00116"}}},"nbformat":4,"nbformat_minor":0}
